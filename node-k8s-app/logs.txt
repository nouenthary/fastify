
==> Audit <==
|------------|------------------|----------|----------------------|---------|---------------------|---------------------|
|  Command   |       Args       | Profile  |         User         | Version |     Start Time      |      End Time       |
|------------|------------------|----------|----------------------|---------|---------------------|---------------------|
| start      |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 12:17 +07 |                     |
| start      |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 12:27 +07 |                     |
| service    | fastify-service  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:24 +07 |                     |
| start      |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:24 +07 | 23 May 25 16:26 +07 |
| service    | fastify-service  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:26 +07 |                     |
| service    | fastify-service  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:27 +07 |                     |
| service    | fastify-service  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:29 +07 |                     |
| service    | fastify-service  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:38 +07 |                     |
| service    | node-app-service | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:42 +07 |                     |
| docker-env |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:45 +07 | 23 May 25 16:45 +07 |
| service    | node-k8s-service | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:47 +07 |                     |
| docker-env |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:52 +07 | 23 May 25 16:52 +07 |
| service    | node-k8s-service | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:52 +07 |                     |
| start      |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:53 +07 | 23 May 25 16:54 +07 |
| docker-env |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:55 +07 | 23 May 25 16:55 +07 |
| docker-env | --shell cmd      | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:55 +07 | 23 May 25 16:55 +07 |
| service    | node-k8s-service | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:57 +07 |                     |
| service    | node-k8s-service | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 16:58 +07 |                     |
| service    | node-k8s-service | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:00 +07 |                     |
| service    | node-k8s-service | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:00 +07 |                     |
| ip         |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:00 +07 | 23 May 25 17:00 +07 |
| start      |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:01 +07 | 23 May 25 17:02 +07 |
| docker-env | --shell cmd      | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:03 +07 | 23 May 25 17:03 +07 |
| ip         |                  | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:04 +07 | 23 May 25 17:04 +07 |
| service    | node-k8s-app     | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:10 +07 |                     |
| service    | node-k8s-service | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:11 +07 |                     |
| docker-env | --shell cmd      | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:12 +07 | 23 May 25 17:12 +07 |
| service    | node-k8s-service | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:15 +07 |                     |
| docker-env | --shell cmd      | minikube | DESKTOP-LFBHHSI\USER | v1.36.0 | 23 May 25 17:15 +07 | 23 May 25 17:15 +07 |
|------------|------------------|----------|----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/23 17:01:12
Running on machine: DESKTOP-LFBHHSI
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0523 17:01:12.747775   15848 out.go:345] Setting OutFile to fd 92 ...
I0523 17:01:12.757797   15848 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0523 17:01:12.757797   15848 out.go:358] Setting ErrFile to fd 96...
I0523 17:01:12.757797   15848 out.go:392] TERM=,COLORTERM=, which probably does not support color
W0523 17:01:12.769212   15848 root.go:314] Error reading config file at C:\Users\USER\.minikube\config\config.json: open C:\Users\USER\.minikube\config\config.json: The system cannot find the file specified.
I0523 17:01:12.772480   15848 out.go:352] Setting JSON to false
I0523 17:01:12.775726   15848 start.go:130] hostinfo: {"hostname":"DESKTOP-LFBHHSI","uptime":11868,"bootTime":1747982604,"procs":246,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.5854 Build 19045.5854","kernelVersion":"10.0.19045.5854 Build 19045.5854","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"23e23cb6-e1f7-4deb-95ab-8f302bb6edfa"}
W0523 17:01:12.775810   15848 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0523 17:01:12.805878   15848 out.go:177] * minikube v1.36.0 on Microsoft Windows 10 Pro 10.0.19045.5854 Build 19045.5854
I0523 17:01:12.880833   15848 notify.go:220] Checking for updates...
I0523 17:01:12.881894   15848 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0523 17:01:12.881894   15848 driver.go:404] Setting default libvirt URI to qemu:///system
I0523 17:01:12.948631   15848 docker.go:123] docker version: linux-28.1.1:Docker Desktop 4.41.2 (191736)
I0523 17:01:12.957647   15848 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0523 17:01:13.170871   15848 info.go:266] docker info: {ID:87287122-34cb-410b-93a3-0ad68af815fb Containers:41 ContainersRunning:24 ContainersPaused:0 ContainersStopped:17 Images:15 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:225 OomKillDisable:true NGoroutines:322 SystemTime:2025-05-23 10:01:13.160204199 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8250175488 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:ufudiq8pck1sgxvcuzs6z7l9q NodeAddr:192.168.65.3 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.3:2377 NodeID:ufudiq8pck1sgxvcuzs6z7l9q]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Users\USER\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.36.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Err:failed to fetch metadata: fork/exec C:\Users\USER\.docker\cli-plugins\docker-feedback.exe: The system cannot find the file specified. Name:feedback Path:C:\Users\USER\.docker\cli-plugins\docker-feedback.exe] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Users\USER\.docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-model.exe] ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.23] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\USER\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.0]] Warnings:<nil>}}
I0523 17:01:13.191336   15848 out.go:177] * Using the docker driver based on existing profile
I0523 17:01:13.212641   15848 start.go:304] selected driver: docker
I0523 17:01:13.212641   15848 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\USER:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0523 17:01:13.212641   15848 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0523 17:01:13.230156   15848 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0523 17:01:13.495501   15848 info.go:266] docker info: {ID:87287122-34cb-410b-93a3-0ad68af815fb Containers:41 ContainersRunning:24 ContainersPaused:0 ContainersStopped:17 Images:15 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:225 OomKillDisable:true NGoroutines:322 SystemTime:2025-05-23 10:01:13.484078221 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8250175488 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:ufudiq8pck1sgxvcuzs6z7l9q NodeAddr:192.168.65.3 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.65.3:2377 NodeID:ufudiq8pck1sgxvcuzs6z7l9q]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Users\USER\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.36.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Err:failed to fetch metadata: fork/exec C:\Users\USER\.docker\cli-plugins\docker-feedback.exe: The system cannot find the file specified. Name:feedback Path:C:\Users\USER\.docker\cli-plugins\docker-feedback.exe] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Users\USER\.docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-model.exe] ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.23] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\USER\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.0]] Warnings:<nil>}}
I0523 17:01:13.575231   15848 cni.go:84] Creating CNI manager for ""
I0523 17:01:13.575231   15848 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0523 17:01:13.575231   15848 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\USER:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0523 17:01:13.607458   15848 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0523 17:01:13.627351   15848 cache.go:121] Beginning downloading kic base image for docker with docker
I0523 17:01:13.680955   15848 out.go:177] * Pulling base image v0.0.47 ...
I0523 17:01:13.693045   15848 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0523 17:01:13.703077   15848 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0523 17:01:13.703141   15848 preload.go:146] Found local preload: C:\Users\USER\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0523 17:01:13.703141   15848 cache.go:56] Caching tarball of preloaded images
I0523 17:01:13.703688   15848 preload.go:172] Found C:\Users\USER\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0523 17:01:13.703688   15848 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0523 17:01:13.703688   15848 profile.go:143] Saving config to C:\Users\USER\.minikube\profiles\minikube\config.json ...
I0523 17:01:13.907699   15848 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0523 17:01:13.907699   15848 localpath.go:146] windows sanitize: C:\Users\USER\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\USER\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0523 17:01:13.908237   15848 localpath.go:146] windows sanitize: C:\Users\USER\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\USER\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0523 17:01:13.908237   15848 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0523 17:01:13.908237   15848 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory, skipping pull
I0523 17:01:13.908237   15848 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in cache, skipping pull
I0523 17:01:13.908237   15848 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0523 17:01:13.908237   15848 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0523 17:01:13.908237   15848 localpath.go:146] windows sanitize: C:\Users\USER\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\USER\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0523 17:01:53.188761   15848 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0523 17:01:53.188761   15848 cache.go:230] Successfully downloaded all kic artifacts
I0523 17:01:53.189299   15848 start.go:360] acquireMachinesLock for minikube: {Name:mkf9b2c0e3eda3858844b01c9a7f8988b59f66a7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0523 17:01:53.189438   15848 start.go:364] duration metric: took 7.5µs to acquireMachinesLock for "minikube"
I0523 17:01:53.189438   15848 start.go:96] Skipping create...Using existing machine configuration
I0523 17:01:53.189438   15848 fix.go:54] fixHost starting: 
I0523 17:01:53.225334   15848 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 17:01:53.267949   15848 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0523 17:01:53.268454   15848 fix.go:138] unexpected machine state, will restart: <nil>
I0523 17:01:53.334777   15848 out.go:177] * Updating the running docker "minikube" container ...
I0523 17:01:53.370606   15848 machine.go:93] provisionDockerMachine start ...
I0523 17:01:53.382716   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:53.446771   15848 main.go:141] libmachine: Using SSH client type: native
I0523 17:01:53.447328   15848 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x87a9e0] 0x87d520 <nil>  [] 0s} 127.0.0.1 63861 <nil> <nil>}
I0523 17:01:53.447328   15848 main.go:141] libmachine: About to run SSH command:
hostname
I0523 17:01:53.599651   15848 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0523 17:01:53.599651   15848 ubuntu.go:169] provisioning hostname "minikube"
I0523 17:01:53.613190   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:53.678148   15848 main.go:141] libmachine: Using SSH client type: native
I0523 17:01:53.678673   15848 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x87a9e0] 0x87d520 <nil>  [] 0s} 127.0.0.1 63861 <nil> <nil>}
I0523 17:01:53.678673   15848 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0523 17:01:53.846305   15848 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0523 17:01:53.858316   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:53.923651   15848 main.go:141] libmachine: Using SSH client type: native
I0523 17:01:53.924194   15848 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x87a9e0] 0x87d520 <nil>  [] 0s} 127.0.0.1 63861 <nil> <nil>}
I0523 17:01:53.924194   15848 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0523 17:01:54.092229   15848 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0523 17:01:54.092229   15848 ubuntu.go:175] set auth options {CertDir:C:\Users\USER\.minikube CaCertPath:C:\Users\USER\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\USER\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\USER\.minikube\machines\server.pem ServerKeyPath:C:\Users\USER\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\USER\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\USER\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\USER\.minikube}
I0523 17:01:54.092229   15848 ubuntu.go:177] setting up certificates
I0523 17:01:54.092229   15848 provision.go:84] configureAuth start
I0523 17:01:54.117824   15848 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0523 17:01:54.185923   15848 provision.go:143] copyHostCerts
I0523 17:01:54.185923   15848 exec_runner.go:144] found C:\Users\USER\.minikube/ca.pem, removing ...
I0523 17:01:54.185923   15848 exec_runner.go:203] rm: C:\Users\USER\.minikube\ca.pem
I0523 17:01:54.186454   15848 exec_runner.go:151] cp: C:\Users\USER\.minikube\certs\ca.pem --> C:\Users\USER\.minikube/ca.pem (1070 bytes)
I0523 17:01:54.186988   15848 exec_runner.go:144] found C:\Users\USER\.minikube/cert.pem, removing ...
I0523 17:01:54.186988   15848 exec_runner.go:203] rm: C:\Users\USER\.minikube\cert.pem
I0523 17:01:54.187491   15848 exec_runner.go:151] cp: C:\Users\USER\.minikube\certs\cert.pem --> C:\Users\USER\.minikube/cert.pem (1115 bytes)
I0523 17:01:54.188093   15848 exec_runner.go:144] found C:\Users\USER\.minikube/key.pem, removing ...
I0523 17:01:54.188093   15848 exec_runner.go:203] rm: C:\Users\USER\.minikube\key.pem
I0523 17:01:54.188093   15848 exec_runner.go:151] cp: C:\Users\USER\.minikube\certs\key.pem --> C:\Users\USER\.minikube/key.pem (1675 bytes)
I0523 17:01:54.188629   15848 provision.go:117] generating server cert: C:\Users\USER\.minikube\machines\server.pem ca-key=C:\Users\USER\.minikube\certs\ca.pem private-key=C:\Users\USER\.minikube\certs\ca-key.pem org=USER.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0523 17:01:54.695926   15848 provision.go:177] copyRemoteCerts
I0523 17:01:54.705995   15848 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0523 17:01:54.722686   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:54.762055   15848 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63861 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0523 17:01:54.866557   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\machines\server.pem --> /etc/docker/server.pem (1172 bytes)
I0523 17:01:54.895264   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0523 17:01:54.918795   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0523 17:01:54.942005   15848 provision.go:87] duration metric: took 849.776ms to configureAuth
I0523 17:01:54.942005   15848 ubuntu.go:193] setting minikube options for container-runtime
I0523 17:01:54.942572   15848 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0523 17:01:54.950110   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:54.994234   15848 main.go:141] libmachine: Using SSH client type: native
I0523 17:01:54.994234   15848 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x87a9e0] 0x87d520 <nil>  [] 0s} 127.0.0.1 63861 <nil> <nil>}
I0523 17:01:54.994234   15848 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0523 17:01:55.128509   15848 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0523 17:01:55.128509   15848 ubuntu.go:71] root file system type: overlay
I0523 17:01:55.128509   15848 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0523 17:01:55.136483   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:55.191041   15848 main.go:141] libmachine: Using SSH client type: native
I0523 17:01:55.191614   15848 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x87a9e0] 0x87d520 <nil>  [] 0s} 127.0.0.1 63861 <nil> <nil>}
I0523 17:01:55.191614   15848 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0523 17:01:55.339489   15848 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0523 17:01:55.348036   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:55.390699   15848 main.go:141] libmachine: Using SSH client type: native
I0523 17:01:55.390699   15848 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x87a9e0] 0x87d520 <nil>  [] 0s} 127.0.0.1 63861 <nil> <nil>}
I0523 17:01:55.390699   15848 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0523 17:01:55.537459   15848 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0523 17:01:55.537459   15848 machine.go:96] duration metric: took 2.1668521s to provisionDockerMachine
I0523 17:01:55.537459   15848 start.go:293] postStartSetup for "minikube" (driver="docker")
I0523 17:01:55.537459   15848 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0523 17:01:55.547660   15848 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0523 17:01:55.555295   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:55.591618   15848 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63861 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0523 17:01:55.711655   15848 ssh_runner.go:195] Run: cat /etc/os-release
I0523 17:01:55.719132   15848 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0523 17:01:55.719132   15848 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0523 17:01:55.719132   15848 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0523 17:01:55.719132   15848 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0523 17:01:55.719132   15848 filesync.go:126] Scanning C:\Users\USER\.minikube\addons for local assets ...
I0523 17:01:55.719687   15848 filesync.go:126] Scanning C:\Users\USER\.minikube\files for local assets ...
I0523 17:01:55.719687   15848 start.go:296] duration metric: took 182.2289ms for postStartSetup
I0523 17:01:55.734924   15848 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0523 17:01:55.744168   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:55.799516   15848 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63861 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0523 17:01:55.919142   15848 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0523 17:01:55.926454   15848 fix.go:56] duration metric: took 2.7370156s for fixHost
I0523 17:01:55.926454   15848 start.go:83] releasing machines lock for "minikube", held for 2.7370156s
I0523 17:01:55.935829   15848 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0523 17:01:55.988882   15848 ssh_runner.go:195] Run: cat /version.json
I0523 17:01:55.989396   15848 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0523 17:01:55.998554   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:56.000316   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:01:56.041139   15848 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63861 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0523 17:01:56.070982   15848 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63861 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0523 17:01:56.159197   15848 ssh_runner.go:195] Run: systemctl --version
W0523 17:01:56.163952   15848 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
W0523 17:01:56.163952   15848 out.go:270] ! Failing to connect to https://registry.k8s.io/ from both inside the minikube container and host machine
W0523 17:01:56.163952   15848 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0523 17:01:56.177544   15848 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0523 17:01:56.195243   15848 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0523 17:01:56.207574   15848 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0523 17:01:56.218700   15848 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0523 17:01:56.233742   15848 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0523 17:01:56.233742   15848 start.go:495] detecting cgroup driver to use...
I0523 17:01:56.233742   15848 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0523 17:01:56.234261   15848 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0523 17:01:56.281428   15848 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0523 17:01:56.319611   15848 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0523 17:01:56.333972   15848 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0523 17:01:56.352135   15848 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0523 17:01:56.374200   15848 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0523 17:01:56.403878   15848 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0523 17:01:56.432389   15848 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0523 17:01:56.461704   15848 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0523 17:01:56.488738   15848 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0523 17:01:56.516865   15848 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0523 17:01:56.544683   15848 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0523 17:01:56.574303   15848 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0523 17:01:56.599431   15848 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0523 17:01:56.627779   15848 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 17:01:56.778835   15848 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0523 17:02:07.409288   15848 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.6304538s)
I0523 17:02:07.409288   15848 start.go:495] detecting cgroup driver to use...
I0523 17:02:07.409288   15848 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0523 17:02:07.423945   15848 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0523 17:02:07.438656   15848 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0523 17:02:07.451802   15848 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0523 17:02:07.467776   15848 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0523 17:02:07.497653   15848 ssh_runner.go:195] Run: which cri-dockerd
I0523 17:02:07.514071   15848 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0523 17:02:07.524790   15848 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0523 17:02:07.553349   15848 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0523 17:02:07.691241   15848 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0523 17:02:07.819412   15848 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0523 17:02:07.819412   15848 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0523 17:02:07.848914   15848 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0523 17:02:07.872682   15848 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 17:02:07.995032   15848 ssh_runner.go:195] Run: sudo systemctl restart docker
I0523 17:02:15.542091   15848 ssh_runner.go:235] Completed: sudo systemctl restart docker: (7.5460582s)
I0523 17:02:15.575759   15848 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0523 17:02:15.644497   15848 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0523 17:02:15.679141   15848 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0523 17:02:15.711580   15848 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0523 17:02:15.881037   15848 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0523 17:02:16.024342   15848 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 17:02:16.156494   15848 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0523 17:02:16.180631   15848 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0523 17:02:16.202831   15848 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 17:02:16.327397   15848 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0523 17:02:16.469253   15848 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0523 17:02:16.484285   15848 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0523 17:02:16.498130   15848 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0523 17:02:16.504129   15848 start.go:563] Will wait 60s for crictl version
I0523 17:02:16.518474   15848 ssh_runner.go:195] Run: which crictl
I0523 17:02:16.542497   15848 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0523 17:02:16.595684   15848 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0523 17:02:16.607920   15848 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0523 17:02:16.645526   15848 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0523 17:02:16.777092   15848 out.go:235] * Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0523 17:02:16.790004   15848 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0523 17:02:16.899907   15848 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0523 17:02:16.911643   15848 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0523 17:02:16.923581   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0523 17:02:16.995723   15848 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\USER:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0523 17:02:16.995723   15848 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0523 17:02:17.005120   15848 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0523 17:02:17.027514   15848 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0523 17:02:17.027514   15848 docker.go:632] Images already preloaded, skipping extraction
I0523 17:02:17.036902   15848 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0523 17:02:17.058243   15848 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0523 17:02:17.058243   15848 cache_images.go:84] Images are preloaded, skipping loading
I0523 17:02:17.058243   15848 kubeadm.go:926] updating node { 192.168.58.2 8443 v1.33.1 docker true true} ...
I0523 17:02:17.058243   15848 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0523 17:02:17.085217   15848 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0523 17:02:17.282341   15848 cni.go:84] Creating CNI manager for ""
I0523 17:02:17.282341   15848 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0523 17:02:17.282861   15848 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0523 17:02:17.282890   15848 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0523 17:02:17.282890   15848 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.58.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0523 17:02:17.298583   15848 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0523 17:02:17.319871   15848 binaries.go:44] Found k8s binaries, skipping transfer
I0523 17:02:17.338630   15848 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0523 17:02:17.352737   15848 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0523 17:02:17.380162   15848 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0523 17:02:17.406976   15848 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0523 17:02:17.460852   15848 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0523 17:02:17.483706   15848 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 17:02:17.653666   15848 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0523 17:02:17.673509   15848 certs.go:68] Setting up C:\Users\USER\.minikube\profiles\minikube for IP: 192.168.58.2
I0523 17:02:17.674029   15848 certs.go:194] generating shared ca certs ...
I0523 17:02:17.674042   15848 certs.go:226] acquiring lock for ca certs: {Name:mkc83777d01c7a642f9f4b7b50df4b1d932d95c7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0523 17:02:17.674568   15848 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\USER\.minikube\ca.key
I0523 17:02:17.674590   15848 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\USER\.minikube\proxy-client-ca.key
I0523 17:02:17.674590   15848 certs.go:256] generating profile certs ...
I0523 17:02:17.675138   15848 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\USER\.minikube\profiles\minikube\client.key
I0523 17:02:17.675138   15848 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\USER\.minikube\profiles\minikube\apiserver.key.502bbb95
I0523 17:02:17.675671   15848 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\USER\.minikube\profiles\minikube\proxy-client.key
I0523 17:02:17.676307   15848 certs.go:484] found cert: C:\Users\USER\.minikube\certs\ca-key.pem (1675 bytes)
I0523 17:02:17.676828   15848 certs.go:484] found cert: C:\Users\USER\.minikube\certs\ca.pem (1070 bytes)
I0523 17:02:17.676969   15848 certs.go:484] found cert: C:\Users\USER\.minikube\certs\cert.pem (1115 bytes)
I0523 17:02:17.676969   15848 certs.go:484] found cert: C:\Users\USER\.minikube\certs\key.pem (1675 bytes)
I0523 17:02:17.678102   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0523 17:02:17.712459   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0523 17:02:17.752879   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0523 17:02:17.801522   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0523 17:02:17.844155   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0523 17:02:17.871244   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0523 17:02:17.892721   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0523 17:02:17.916100   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0523 17:02:17.938940   15848 ssh_runner.go:362] scp C:\Users\USER\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0523 17:02:17.971305   15848 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0523 17:02:17.998349   15848 ssh_runner.go:195] Run: openssl version
I0523 17:02:18.015155   15848 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0523 17:02:18.036387   15848 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0523 17:02:18.041272   15848 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 23 05:28 /usr/share/ca-certificates/minikubeCA.pem
I0523 17:02:18.051156   15848 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0523 17:02:18.069804   15848 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0523 17:02:18.089709   15848 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0523 17:02:18.105495   15848 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0523 17:02:18.124273   15848 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0523 17:02:18.141682   15848 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0523 17:02:18.159359   15848 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0523 17:02:18.177133   15848 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0523 17:02:18.194276   15848 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0523 17:02:18.202556   15848 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\USER:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0523 17:02:18.211708   15848 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0523 17:02:18.240198   15848 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0523 17:02:18.250297   15848 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0523 17:02:18.250297   15848 kubeadm.go:589] restartPrimaryControlPlane start ...
I0523 17:02:18.261261   15848 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0523 17:02:18.270248   15848 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0523 17:02:18.279938   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0523 17:02:18.341145   15848 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:63865"
I0523 17:02:18.353479   15848 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0523 17:02:18.363572   15848 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0523 17:02:18.363572   15848 kubeadm.go:593] duration metric: took 113.2754ms to restartPrimaryControlPlane
I0523 17:02:18.363572   15848 kubeadm.go:394] duration metric: took 161.0158ms to StartCluster
I0523 17:02:18.363572   15848 settings.go:142] acquiring lock: {Name:mkcd282b810ea7bce3652a3109f04f5e9f9f01ff Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0523 17:02:18.363572   15848 settings.go:150] Updating kubeconfig:  C:\Users\USER\.kube\config
I0523 17:02:18.364640   15848 lock.go:35] WriteFile acquiring C:\Users\USER\.kube\config: {Name:mk337b1533ed7fcb58cddea65bf7e3ba0a1c487e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0523 17:02:18.365535   15848 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0523 17:02:18.365535   15848 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0523 17:02:18.365535   15848 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0523 17:02:18.365535   15848 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0523 17:02:18.365535   15848 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0523 17:02:18.365535   15848 addons.go:247] addon storage-provisioner should already be in state true
I0523 17:02:18.365535   15848 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0523 17:02:18.365535   15848 host.go:66] Checking if "minikube" exists ...
I0523 17:02:18.365535   15848 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0523 17:02:18.386120   15848 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 17:02:18.387255   15848 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 17:02:18.439884   15848 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0523 17:02:18.461601   15848 addons.go:247] addon default-storageclass should already be in state true
I0523 17:02:18.460891   15848 out.go:177] * Verifying Kubernetes components...
I0523 17:02:18.461601   15848 host.go:66] Checking if "minikube" exists ...
I0523 17:02:18.477691   15848 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0523 17:02:18.549916   15848 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0523 17:02:18.610938   15848 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0523 17:02:18.610938   15848 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0523 17:02:18.615788   15848 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0523 17:02:18.619482   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:02:18.645443   15848 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0523 17:02:18.645443   15848 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0523 17:02:18.655016   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0523 17:02:18.659829   15848 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63861 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0523 17:02:18.711853   15848 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63861 SSHKeyPath:C:\Users\USER\.minikube\machines\minikube\id_rsa Username:docker}
I0523 17:02:18.756530   15848 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0523 17:02:18.778100   15848 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0523 17:02:18.782358   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0523 17:02:18.833144   15848 api_server.go:52] waiting for apiserver process to appear ...
I0523 17:02:18.834202   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0523 17:02:18.850197   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0523 17:02:18.881406   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:18.881406   15848 retry.go:31] will retry after 126.649655ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0523 17:02:18.924838   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:18.924838   15848 retry.go:31] will retry after 308.189743ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:19.023066   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0523 17:02:19.118030   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:19.118030   15848 retry.go:31] will retry after 299.765683ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:19.251561   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0523 17:02:19.350502   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:19.350502   15848 retry.go:31] will retry after 532.563845ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:19.354547   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:19.433532   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0523 17:02:19.535179   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:19.535179   15848 retry.go:31] will retry after 765.093439ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:19.846294   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:19.898875   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0523 17:02:20.025388   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:20.025388   15848 retry.go:31] will retry after 746.243026ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:20.316258   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0523 17:02:20.350194   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0523 17:02:20.408436   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:20.408436   15848 retry.go:31] will retry after 795.829395ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:20.786664   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0523 17:02:20.848483   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0523 17:02:20.867489   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:20.867489   15848 retry.go:31] will retry after 965.73767ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:21.220695   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0523 17:02:21.306065   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:21.306065   15848 retry.go:31] will retry after 1.568977882s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:21.346936   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:21.847014   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:21.847014   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0523 17:02:21.929039   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:21.929039   15848 retry.go:31] will retry after 1.304427223s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:22.355491   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:22.859296   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:22.897843   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0523 17:02:22.978991   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:22.978991   15848 retry.go:31] will retry after 2.209299747s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:23.247866   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0523 17:02:23.330700   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:23.330700   15848 retry.go:31] will retry after 1.864290684s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:23.347560   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:23.846725   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:24.352989   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:24.857286   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:25.212511   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0523 17:02:25.222349   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0523 17:02:25.303861   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:25.303861   15848 retry.go:31] will retry after 2.761577175s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0523 17:02:25.337525   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:25.337525   15848 retry.go:31] will retry after 2.537776994s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:25.351023   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:25.844553   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:26.349439   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:26.855577   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:27.345593   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:27.844878   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:27.885542   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0523 17:02:27.940677   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:27.940677   15848 retry.go:31] will retry after 2.983244128s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:28.081127   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0523 17:02:28.148046   15848 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:28.148046   15848 retry.go:31] will retry after 5.559375s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0523 17:02:28.347715   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:28.857376   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:29.352689   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:29.849092   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:30.349155   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:30.850777   15848 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0523 17:02:30.869340   15848 api_server.go:72] duration metric: took 12.5038057s to wait for apiserver process to appear ...
I0523 17:02:30.869340   15848 api_server.go:88] waiting for apiserver healthz status ...
I0523 17:02:30.869884   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:30.938788   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0523 17:02:33.721714   15848 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0523 17:02:35.870204   15848 api_server.go:269] stopped: https://127.0.0.1:63865/healthz: Get "https://127.0.0.1:63865/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0523 17:02:35.870204   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:36.199949   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W0523 17:02:36.199949   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I0523 17:02:36.370051   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:36.377939   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:36.377939   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:36.869700   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:36.892248   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:36.892248   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:37.369882   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:37.378248   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:37.378248   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:37.729184   15848 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (4.0074704s)
I0523 17:02:37.729184   15848 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (6.7903958s)
I0523 17:02:37.869515   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:37.878381   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:37.878381   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:38.103573   15848 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0523 17:02:38.210166   15848 addons.go:514] duration metric: took 19.8433059s for enable addons: enabled=[storage-provisioner default-storageclass]
I0523 17:02:38.369640   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:38.378219   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:38.378219   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:38.870623   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:38.884732   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:38.884732   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:39.369986   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:39.380050   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:39.380050   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:39.869927   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:39.878597   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:39.878597   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:40.369596   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:40.380729   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:40.380729   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:40.869952   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:40.885401   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:40.885401   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:41.370149   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:41.389946   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:41.389946   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:41.869794   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:41.878480   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:41.878480   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:42.369914   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:42.437644   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:42.437644   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:42.869421   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:42.881460   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0523 17:02:42.881460   15848 api_server.go:103] status: https://127.0.0.1:63865/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0523 17:02:43.369890   15848 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63865/healthz ...
I0523 17:02:43.565538   15848 api_server.go:279] https://127.0.0.1:63865/healthz returned 200:
ok
I0523 17:02:43.569610   15848 api_server.go:141] control plane version: v1.33.1
I0523 17:02:43.569610   15848 api_server.go:131] duration metric: took 12.7002697s to wait for apiserver health ...
I0523 17:02:43.569610   15848 system_pods.go:43] waiting for kube-system pods to appear ...
I0523 17:02:43.619819   15848 system_pods.go:59] 8 kube-system pods found
I0523 17:02:43.619819   15848 system_pods.go:61] "coredns-674b8bbfcf-6c2s8" [de8b95ac-1690-4489-9496-f2327413ae4f] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0523 17:02:43.619819   15848 system_pods.go:61] "coredns-674b8bbfcf-7w9x5" [35b503c9-a5c9-417f-a71f-758ec394b981] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0523 17:02:43.619819   15848 system_pods.go:61] "etcd-minikube" [f7d4c4ef-3831-4bec-8a25-b7d3c02a5385] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0523 17:02:43.619819   15848 system_pods.go:61] "kube-apiserver-minikube" [f73f4ad1-1135-4039-8d59-a4de1b47e215] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0523 17:02:43.619819   15848 system_pods.go:61] "kube-controller-manager-minikube" [662865c1-e001-4af2-8083-1d9671d201b7] Running
I0523 17:02:43.619819   15848 system_pods.go:61] "kube-proxy-p8gbp" [d3651d6a-ba95-40b8-a963-5085cc79bab1] Running
I0523 17:02:43.619819   15848 system_pods.go:61] "kube-scheduler-minikube" [5ed4531c-ab18-4693-895c-90276e43beaf] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0523 17:02:43.619819   15848 system_pods.go:61] "storage-provisioner" [1a1b96f0-3428-429f-a206-ea0c9865c6d5] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0523 17:02:43.619819   15848 system_pods.go:74] duration metric: took 50.209ms to wait for pod list to return data ...
I0523 17:02:43.619819   15848 kubeadm.go:578] duration metric: took 25.2542844s to wait for: map[apiserver:true system_pods:true]
I0523 17:02:43.619819   15848 node_conditions.go:102] verifying NodePressure condition ...
I0523 17:02:43.663181   15848 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0523 17:02:43.663181   15848 node_conditions.go:123] node cpu capacity is 8
I0523 17:02:43.663181   15848 node_conditions.go:105] duration metric: took 43.3614ms to run NodePressure ...
I0523 17:02:43.663181   15848 start.go:241] waiting for startup goroutines ...
I0523 17:02:43.663181   15848 start.go:246] waiting for cluster config update ...
I0523 17:02:43.663181   15848 start.go:255] writing updated cluster config ...
I0523 17:02:43.674555   15848 ssh_runner.go:195] Run: rm -f paused
I0523 17:02:43.787737   15848 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0523 17:02:43.885603   15848 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 23 10:09:46 minikube cri-dockerd[18568]: time="2025-05-23T10:09:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/848d712039354def7965742373c4b530248b0825185af127787013fa90858698/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 10:09:47 minikube cri-dockerd[18568]: time="2025-05-23T10:09:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/85f859aaf4477a1ee220cbb88fd4f9a38ed217ebc85fc43bf226c23bda0b2882/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 10:09:49 minikube dockerd[18194]: time="2025-05-23T10:09:49.229183709Z" level=info msg="ignoring event" container=033507be1d86bdf61c3af2f1560938d9085b6bfa8f8f1b2fd651cd77e06131d7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 10:09:49 minikube dockerd[18194]: time="2025-05-23T10:09:49.549271540Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:09:49 minikube dockerd[18194]: time="2025-05-23T10:09:49.549380146Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:09:51 minikube dockerd[18194]: time="2025-05-23T10:09:51.312309210Z" level=info msg="ignoring event" container=5e5472e083494a9d4d9decb78549d6f950e7fbdd36eb607dc0f4bd166204ac78 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 10:09:53 minikube dockerd[18194]: time="2025-05-23T10:09:53.227619580Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:09:53 minikube dockerd[18194]: time="2025-05-23T10:09:53.227708585Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:09:54 minikube dockerd[18194]: time="2025-05-23T10:09:54.116724325Z" level=info msg="ignoring event" container=848d712039354def7965742373c4b530248b0825185af127787013fa90858698 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 10:09:56 minikube dockerd[18194]: time="2025-05-23T10:09:56.814193604Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:09:56 minikube dockerd[18194]: time="2025-05-23T10:09:56.814285710Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:09:58 minikube dockerd[18194]: time="2025-05-23T10:09:58.087190066Z" level=info msg="ignoring event" container=052c6efcd4bfb6ba7a37962f239e2af554adb832851a8bed5a5ebe8d77ed4679 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 10:10:00 minikube dockerd[18194]: time="2025-05-23T10:10:00.593210318Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:10:00 minikube dockerd[18194]: time="2025-05-23T10:10:00.593274222Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:10:02 minikube dockerd[18194]: time="2025-05-23T10:10:02.133011946Z" level=info msg="ignoring event" container=85f859aaf4477a1ee220cbb88fd4f9a38ed217ebc85fc43bf226c23bda0b2882 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 10:10:09 minikube cri-dockerd[18568]: time="2025-05-23T10:10:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/37bf4efaa2985757ae4fb48b9473b4ee2acac431c766b389cf14fb6767bbe5d4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 10:10:10 minikube cri-dockerd[18568]: time="2025-05-23T10:10:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/376517f0a1b593574cfa2f8bbd00e6efe6d4bbb7804afd52e34493127baaf4bb/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 10:10:12 minikube dockerd[18194]: time="2025-05-23T10:10:12.804039636Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:10:12 minikube dockerd[18194]: time="2025-05-23T10:10:12.804143742Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:10:16 minikube dockerd[18194]: time="2025-05-23T10:10:16.359299827Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:10:16 minikube dockerd[18194]: time="2025-05-23T10:10:16.359368932Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:10:31 minikube dockerd[18194]: time="2025-05-23T10:10:31.505849354Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:10:31 minikube dockerd[18194]: time="2025-05-23T10:10:31.505940762Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:10:35 minikube dockerd[18194]: time="2025-05-23T10:10:35.198848879Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:10:35 minikube dockerd[18194]: time="2025-05-23T10:10:35.198940385Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:10:58 minikube dockerd[18194]: time="2025-05-23T10:10:58.313710911Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:10:58 minikube dockerd[18194]: time="2025-05-23T10:10:58.313794716Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:11:04 minikube dockerd[18194]: time="2025-05-23T10:11:04.500455402Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:11:04 minikube dockerd[18194]: time="2025-05-23T10:11:04.500541508Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:11:43 minikube dockerd[18194]: time="2025-05-23T10:11:43.524702056Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:11:43 minikube dockerd[18194]: time="2025-05-23T10:11:43.524812663Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:11:50 minikube dockerd[18194]: time="2025-05-23T10:11:50.384518778Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:11:50 minikube dockerd[18194]: time="2025-05-23T10:11:50.384644586Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:13:15 minikube dockerd[18194]: time="2025-05-23T10:13:15.576569682Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:13:15 minikube dockerd[18194]: time="2025-05-23T10:13:15.576658787Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:13:20 minikube dockerd[18194]: time="2025-05-23T10:13:20.117231130Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:13:20 minikube dockerd[18194]: time="2025-05-23T10:13:20.117300634Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:15:02 minikube dockerd[18194]: time="2025-05-23T10:15:02.735155002Z" level=info msg="ignoring event" container=376517f0a1b593574cfa2f8bbd00e6efe6d4bbb7804afd52e34493127baaf4bb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 10:15:02 minikube dockerd[18194]: time="2025-05-23T10:15:02.774233051Z" level=info msg="ignoring event" container=37bf4efaa2985757ae4fb48b9473b4ee2acac431c766b389cf14fb6767bbe5d4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 23 10:15:09 minikube cri-dockerd[18568]: time="2025-05-23T10:15:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6138ee1693553cbd136bde73ac1ab2683ccb10e3a6cb06370a90a0c9e2be989b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 10:15:10 minikube cri-dockerd[18568]: time="2025-05-23T10:15:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3741fc63bc1a6581ed141eb3d486a4aa6a085c33168d4398c5b19a72f4ace03b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 10:15:12 minikube cri-dockerd[18568]: time="2025-05-23T10:15:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e50c2b80ca090b42cc717b571f52e0c63d6fabee8dae94393f2052ac2f3a9cc0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 23 10:15:13 minikube dockerd[18194]: time="2025-05-23T10:15:13.591481273Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:15:13 minikube dockerd[18194]: time="2025-05-23T10:15:13.591596281Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:15:17 minikube dockerd[18194]: time="2025-05-23T10:15:17.276123519Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:15:17 minikube dockerd[18194]: time="2025-05-23T10:15:17.277079978Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:15:20 minikube dockerd[18194]: time="2025-05-23T10:15:20.952014041Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:15:20 minikube dockerd[18194]: time="2025-05-23T10:15:20.952113448Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:15:30 minikube dockerd[18194]: time="2025-05-23T10:15:30.502433899Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:15:30 minikube dockerd[18194]: time="2025-05-23T10:15:30.502522904Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:15:34 minikube dockerd[18194]: time="2025-05-23T10:15:34.065058077Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:15:34 minikube dockerd[18194]: time="2025-05-23T10:15:34.065155484Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:15:38 minikube dockerd[18194]: time="2025-05-23T10:15:38.381310233Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:15:38 minikube dockerd[18194]: time="2025-05-23T10:15:38.381414239Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:15:59 minikube dockerd[18194]: time="2025-05-23T10:15:59.461274492Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:15:59 minikube dockerd[18194]: time="2025-05-23T10:15:59.461374999Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:16:03 minikube dockerd[18194]: time="2025-05-23T10:16:03.468685232Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:16:03 minikube dockerd[18194]: time="2025-05-23T10:16:03.468797340Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 23 10:16:09 minikube dockerd[18194]: time="2025-05-23T10:16:09.472721054Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 23 10:16:09 minikube dockerd[18194]: time="2025-05-23T10:16:09.472819960Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
9d1a23583570d       6e38f40d628db       13 minutes ago      Running             storage-provisioner       4                   8221526f85e6d       storage-provisioner
435187e5c7bc3       b79c189b052cd       14 minutes ago      Running             kube-proxy                2                   27d236d312469       kube-proxy-p8gbp
6cd0436c58184       499038711c081       14 minutes ago      Running             etcd                      2                   42bcd997ec15e       etcd-minikube
0988443c33da7       ef43894fa110c       14 minutes ago      Running             kube-controller-manager   3                   a1f6fdae57cee       kube-controller-manager-minikube
7e0e36f2a8ede       1cf5f116067c6       14 minutes ago      Running             coredns                   2                   06eb96c2fbf25       coredns-674b8bbfcf-6c2s8
3556ea0972889       1cf5f116067c6       14 minutes ago      Running             coredns                   2                   b7ab764f2ab98       coredns-674b8bbfcf-7w9x5
95cdf6db8c85d       c6ab243b29f82       14 minutes ago      Running             kube-apiserver            2                   2315daf650c53       kube-apiserver-minikube
231c33f225087       6e38f40d628db       14 minutes ago      Exited              storage-provisioner       3                   8221526f85e6d       storage-provisioner
1a01f895e229b       398c985c0d950       14 minutes ago      Running             kube-scheduler            2                   8d9c84a2a648b       kube-scheduler-minikube
0520697c07e05       ef43894fa110c       22 minutes ago      Exited              kube-controller-manager   2                   59932493a26d7       kube-controller-manager-minikube
12fcd0327417e       1cf5f116067c6       22 minutes ago      Exited              coredns                   1                   28590b6a0b1cb       coredns-674b8bbfcf-6c2s8
66ee3596c08f9       b79c189b052cd       22 minutes ago      Exited              kube-proxy                1                   bf6ceed9b4aba       kube-proxy-p8gbp
c632776288201       1cf5f116067c6       22 minutes ago      Exited              coredns                   1                   93f3cd6090428       coredns-674b8bbfcf-7w9x5
80f7b28aabb8b       499038711c081       22 minutes ago      Exited              etcd                      1                   d85d420b3008d       etcd-minikube
6124a194e1e38       c6ab243b29f82       22 minutes ago      Exited              kube-apiserver            1                   717daec4d1ac8       kube-apiserver-minikube
4371e7b3876d9       398c985c0d950       22 minutes ago      Exited              kube-scheduler            1                   a05e66cd316e7       kube-scheduler-minikube


==> coredns [12fcd0327417] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [3556ea097288] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> coredns [7e0e36f2a8ed] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1


==> coredns [c63277628820] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_23T16_26_19_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 23 May 2025 09:25:55 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 23 May 2025 10:16:46 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 23 May 2025 10:12:47 +0000   Fri, 23 May 2025 09:25:55 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 23 May 2025 10:12:47 +0000   Fri, 23 May 2025 09:25:55 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 23 May 2025 10:12:47 +0000   Fri, 23 May 2025 09:25:55 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 23 May 2025 10:12:47 +0000   Fri, 23 May 2025 09:26:01 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8056812Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8056812Ki
  pods:               110
System Info:
  Machine ID:                 fa99a9f228584e52b41e90208cb07947
  System UUID:                fa99a9f228584e52b41e90208cb07947
  Boot ID:                    d7452759-6276-49bf-9f17-1cac9fa75e2b
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     node-k8s-app-6cc8f6c447-npkks       0 (0%)        0 (0%)      0 (0%)           0 (0%)         100s
  default                     node-k8s-app-74d4cb658b-8vj5b       0 (0%)        0 (0%)      0 (0%)           0 (0%)         105s
  default                     node-k8s-app-74d4cb658b-kq64j       0 (0%)        0 (0%)      0 (0%)           0 (0%)         105s
  kube-system                 coredns-674b8bbfcf-6c2s8            100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     50m
  kube-system                 coredns-674b8bbfcf-7w9x5            100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     50m
  kube-system                 etcd-minikube                       100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         50m
  kube-system                 kube-apiserver-minikube             250m (3%)     0 (0%)      0 (0%)           0 (0%)         50m
  kube-system                 kube-controller-manager-minikube    200m (2%)     0 (0%)      0 (0%)           0 (0%)         50m
  kube-system                 kube-proxy-p8gbp                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m
  kube-system                 kube-scheduler-minikube             100m (1%)     0 (0%)      0 (0%)           0 (0%)         50m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%)  0 (0%)
  memory             240Mi (3%)  340Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           50m                kube-proxy       
  Normal   Starting                           14m                kube-proxy       
  Normal   Starting                           22m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  51m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           51m                kubelet          Starting kubelet.
  Warning  CgroupV1                           51m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            51m (x8 over 51m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              51m (x8 over 51m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               51m (x7 over 51m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            51m                kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  50m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           50m                kubelet          Starting kubelet.
  Warning  CgroupV1                           50m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            50m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            50m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              50m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               50m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     50m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     22m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     14m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May23 09:22] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3 #4 #5 #6 #7
[  +0.003905] PCI: Fatal: No config space access function found
[  +0.016883] PCI: System does not support PCI
[  +0.018331] kvm: no hardware support
[  +0.000002] kvm: no hardware support
[  +1.864007] FS-Cache: Duplicate cookie detected
[  +0.000991] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000568] FS-Cache: O-cookie d=00000000625591e7{9P.session} n=00000000e23632c7
[  +0.000556] FS-Cache: O-key=[10] '34323934393337343838'
[  +0.000393] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000516] FS-Cache: N-cookie d=00000000625591e7{9P.session} n=00000000e7e32132
[  +0.000694] FS-Cache: N-key=[10] '34323934393337343838'
[  +0.001773] FS-Cache: Duplicate cookie detected
[  +0.000403] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000717] FS-Cache: O-cookie d=00000000625591e7{9P.session} n=00000000e23632c7
[  +0.000516] FS-Cache: O-key=[10] '34323934393337343838'
[  +0.000411] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000385] FS-Cache: N-cookie d=00000000625591e7{9P.session} n=00000000174eeb2a
[  +0.000498] FS-Cache: N-key=[10] '34323934393337343838'
[  +0.671002] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.008350] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Bangkok not found. Is the tzdata package installed?
[  +0.378149] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000624] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000719] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000670] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.292948] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000453] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000369] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000443] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.463742] Failed to connect to bus: No such file or directory
[  +0.254822] systemd-journald[38]: File /var/log/journal/ff09278ac24f4ff689c71961b64a0db5/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +4.550092] systemd-journald[38]: File /var/log/journal/ff09278ac24f4ff689c71961b64a0db5/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.
[  +1.216398] new mount options do not match the existing superblock, will be ignored
[  +0.002189] netlink: 'init': attribute type 4 has an invalid length.
[ +13.225166] Exception: 
[  +0.000004] Operation canceled @p9io.cpp:258 (AcceptAsync)

[May23 09:23] tmpfs: Unknown parameter 'noswap'
[May23 09:25] tmpfs: Unknown parameter 'noswap'
[May23 09:26] tmpfs: Unknown parameter 'noswap'
[May23 09:40] hrtimer: interrupt took 1642946 ns


==> etcd [6cd0436c5818] <==
{"level":"info","ts":"2025-05-23T10:15:18.116351Z","caller":"traceutil/trace.go:171","msg":"trace[900100949] transaction","detail":"{read_only:false; response_revision:4441; number_of_response:1; }","duration":"130.218931ms","start":"2025-05-23T10:15:17.986119Z","end":"2025-05-23T10:15:18.116338Z","steps":["trace[900100949] 'process raft request'  (duration: 87.310785ms)","trace[900100949] 'compare'  (duration: 42.754537ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:15:18.364204Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"139.200686ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:15:18.364371Z","caller":"traceutil/trace.go:171","msg":"trace[1503156894] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4442; }","duration":"139.393198ms","start":"2025-05-23T10:15:18.224964Z","end":"2025-05-23T10:15:18.364357Z","steps":["trace[1503156894] 'range keys from in-memory index tree'  (duration: 139.153482ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:15:18.364280Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"128.970554ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:15:18.364623Z","caller":"traceutil/trace.go:171","msg":"trace[981218913] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4442; }","duration":"129.329476ms","start":"2025-05-23T10:15:18.235282Z","end":"2025-05-23T10:15:18.364612Z","steps":["trace[981218913] 'range keys from in-memory index tree'  (duration: 128.929453ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:15:21.153549Z","caller":"traceutil/trace.go:171","msg":"trace[2007696543] transaction","detail":"{read_only:false; response_revision:4445; number_of_response:1; }","duration":"127.366556ms","start":"2025-05-23T10:15:21.026149Z","end":"2025-05-23T10:15:21.153515Z","steps":["trace[2007696543] 'process raft request'  (duration: 127.248848ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:15:22.183607Z","caller":"traceutil/trace.go:171","msg":"trace[1669706326] linearizableReadLoop","detail":"{readStateIndex:5107; appliedIndex:5104; }","duration":"162.508032ms","start":"2025-05-23T10:15:22.021083Z","end":"2025-05-23T10:15:22.183591Z","steps":["trace[1669706326] 'read index received'  (duration: 14.156674ms)","trace[1669706326] 'applied index is now lower than readState.Index'  (duration: 148.350858ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:15:22.183739Z","caller":"traceutil/trace.go:171","msg":"trace[857925342] transaction","detail":"{read_only:false; response_revision:4451; number_of_response:1; }","duration":"166.704391ms","start":"2025-05-23T10:15:22.017022Z","end":"2025-05-23T10:15:22.183727Z","steps":["trace[857925342] 'process raft request'  (duration: 166.514779ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:15:22.183890Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"140.705586ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/coredns\" limit:1 ","response":"range_response_count:1 size:179"}
{"level":"info","ts":"2025-05-23T10:15:22.183964Z","caller":"traceutil/trace.go:171","msg":"trace[245798177] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/coredns; range_end:; response_count:1; response_revision:4451; }","duration":"140.813293ms","start":"2025-05-23T10:15:22.043132Z","end":"2025-05-23T10:15:22.183945Z","steps":["trace[245798177] 'agreement among raft nodes before linearized reading'  (duration: 140.670584ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:15:22.183894Z","caller":"traceutil/trace.go:171","msg":"trace[7080571] transaction","detail":"{read_only:false; response_revision:4450; number_of_response:1; }","duration":"166.8453ms","start":"2025-05-23T10:15:22.017022Z","end":"2025-05-23T10:15:22.183868Z","steps":["trace[7080571] 'process raft request'  (duration: 145.100057ms)","trace[7080571] 'compare'  (duration: 21.302515ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:15:22.183903Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"162.80425ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csistoragecapacities/\" range_end:\"/registry/csistoragecapacities0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:15:22.184163Z","caller":"traceutil/trace.go:171","msg":"trace[914099786] range","detail":"{range_begin:/registry/csistoragecapacities/; range_end:/registry/csistoragecapacities0; response_count:0; response_revision:4451; }","duration":"163.088768ms","start":"2025-05-23T10:15:22.021057Z","end":"2025-05-23T10:15:22.184145Z","steps":["trace[914099786] 'agreement among raft nodes before linearized reading'  (duration: 162.79175ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:15:27.091385Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"145.999413ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618684989890 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/node-k8s-app-74d4cb658b-kq64j.18422079193a9508\" mod_revision:4406 > success:<request_put:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-kq64j.18422079193a9508\" value_size:645 lease:3238535618684989570 >> failure:<request_range:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-kq64j.18422079193a9508\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-23T10:15:27.091487Z","caller":"traceutil/trace.go:171","msg":"trace[1005076012] transaction","detail":"{read_only:false; response_revision:4457; number_of_response:1; }","duration":"228.033177ms","start":"2025-05-23T10:15:26.863436Z","end":"2025-05-23T10:15:27.091469Z","steps":["trace[1005076012] 'process raft request'  (duration: 81.896256ms)","trace[1005076012] 'compare'  (duration: 145.818301ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:15:27.132309Z","caller":"traceutil/trace.go:171","msg":"trace[521031586] transaction","detail":"{read_only:false; response_revision:4458; number_of_response:1; }","duration":"264.441424ms","start":"2025-05-23T10:15:26.867850Z","end":"2025-05-23T10:15:27.132292Z","steps":["trace[521031586] 'process raft request'  (duration: 264.323417ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:15:28.965512Z","caller":"traceutil/trace.go:171","msg":"trace[1585013907] transaction","detail":"{read_only:false; response_revision:4460; number_of_response:1; }","duration":"103.372881ms","start":"2025-05-23T10:15:28.862110Z","end":"2025-05-23T10:15:28.965483Z","steps":["trace[1585013907] 'process raft request'  (duration: 103.175969ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:15:29.153437Z","caller":"traceutil/trace.go:171","msg":"trace[1139616635] transaction","detail":"{read_only:false; response_revision:4461; number_of_response:1; }","duration":"288.616617ms","start":"2025-05-23T10:15:28.864793Z","end":"2025-05-23T10:15:29.153410Z","steps":["trace[1139616635] 'process raft request'  (duration: 267.234097ms)","trace[1139616635] 'compare'  (duration: 21.172807ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:15:30.787100Z","caller":"traceutil/trace.go:171","msg":"trace[344216541] transaction","detail":"{read_only:false; response_revision:4464; number_of_response:1; }","duration":"174.337607ms","start":"2025-05-23T10:15:30.612740Z","end":"2025-05-23T10:15:30.787078Z","steps":["trace[344216541] 'process raft request'  (duration: 153.400897ms)","trace[344216541] 'compare'  (duration: 20.7696ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:15:33.191147Z","caller":"traceutil/trace.go:171","msg":"trace[1741569077] transaction","detail":"{read_only:false; response_revision:4466; number_of_response:1; }","duration":"121.836622ms","start":"2025-05-23T10:15:33.069286Z","end":"2025-05-23T10:15:33.191123Z","steps":["trace[1741569077] 'process raft request'  (duration: 42.893883ms)","trace[1741569077] 'compare'  (duration: 78.80973ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:15:35.227396Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.320789ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618684989941 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" mod_revision:4448 > success:<request_put:<key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" value_size:3255 >> failure:<request_range:<key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-23T10:15:35.227610Z","caller":"traceutil/trace.go:171","msg":"trace[1663178625] transaction","detail":"{read_only:false; response_revision:4471; number_of_response:1; }","duration":"362.484478ms","start":"2025-05-23T10:15:34.865102Z","end":"2025-05-23T10:15:35.227587Z","steps":["trace[1663178625] 'process raft request'  (duration: 256.891472ms)","trace[1663178625] 'compare'  (duration: 105.238583ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:15:35.227686Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T10:15:34.865078Z","time spent":"362.569382ms","remote":"127.0.0.1:35814","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3315,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" mod_revision:4448 > success:<request_put:<key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" value_size:3255 >> failure:<request_range:<key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" > >"}
{"level":"info","ts":"2025-05-23T10:15:35.228463Z","caller":"traceutil/trace.go:171","msg":"trace[864386117] transaction","detail":"{read_only:false; response_revision:4472; number_of_response:1; }","duration":"306.807694ms","start":"2025-05-23T10:15:34.921640Z","end":"2025-05-23T10:15:35.228448Z","steps":["trace[864386117] 'process raft request'  (duration: 305.893737ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:15:35.228560Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T10:15:34.921615Z","time spent":"306.891299ms","remote":"127.0.0.1:35884","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:4454 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-05-23T10:15:44.007117Z","caller":"traceutil/trace.go:171","msg":"trace[55022753] transaction","detail":"{read_only:false; response_revision:4482; number_of_response:1; }","duration":"133.251299ms","start":"2025-05-23T10:15:43.873828Z","end":"2025-05-23T10:15:44.007079Z","steps":["trace[55022753] 'process raft request'  (duration: 132.796871ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:15:46.015073Z","caller":"traceutil/trace.go:171","msg":"trace[2123799910] linearizableReadLoop","detail":"{readStateIndex:5148; appliedIndex:5147; }","duration":"126.863081ms","start":"2025-05-23T10:15:45.888180Z","end":"2025-05-23T10:15:46.015043Z","steps":["trace[2123799910] 'read index received'  (duration: 62.447333ms)","trace[2123799910] 'applied index is now lower than readState.Index'  (duration: 64.414848ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:15:46.015339Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.097594ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/node-k8s-app-74d4cb658b-8vj5b.1842207af9373b5d\" limit:1 ","response":"range_response_count:1 size:744"}
{"level":"info","ts":"2025-05-23T10:15:46.015402Z","caller":"traceutil/trace.go:171","msg":"trace[367616589] range","detail":"{range_begin:/registry/events/default/node-k8s-app-74d4cb658b-8vj5b.1842207af9373b5d; range_end:; response_count:1; response_revision:4487; }","duration":"127.238002ms","start":"2025-05-23T10:15:45.888153Z","end":"2025-05-23T10:15:46.015391Z","steps":["trace[367616589] 'agreement among raft nodes before linearized reading'  (duration: 127.03269ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:15:46.015344Z","caller":"traceutil/trace.go:171","msg":"trace[507384763] transaction","detail":"{read_only:false; response_revision:4487; number_of_response:1; }","duration":"147.8204ms","start":"2025-05-23T10:15:45.867461Z","end":"2025-05-23T10:15:46.015282Z","steps":["trace[507384763] 'process raft request'  (duration: 83.211541ms)","trace[507384763] 'compare'  (duration: 64.201935ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:15:53.164634Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"219.283157ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618684990048 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" mod_revision:4471 > success:<request_put:<key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" value_size:3198 >> failure:<request_range:<key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-23T10:15:53.164790Z","caller":"traceutil/trace.go:171","msg":"trace[580483294] linearizableReadLoop","detail":"{readStateIndex:5158; appliedIndex:5156; }","duration":"120.987238ms","start":"2025-05-23T10:15:53.043790Z","end":"2025-05-23T10:15:53.164777Z","steps":["trace[580483294] 'read index received'  (duration: 26.538544ms)","trace[580483294] 'applied index is now lower than readState.Index'  (duration: 94.447894ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:15:53.164871Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.072343ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.58.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-05-23T10:15:53.164898Z","caller":"traceutil/trace.go:171","msg":"trace[590754939] range","detail":"{range_begin:/registry/masterleases/192.168.58.2; range_end:; response_count:1; response_revision:4495; }","duration":"121.131347ms","start":"2025-05-23T10:15:53.043759Z","end":"2025-05-23T10:15:53.164890Z","steps":["trace[590754939] 'agreement among raft nodes before linearized reading'  (duration: 121.059443ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:15:53.164909Z","caller":"traceutil/trace.go:171","msg":"trace[1518140724] transaction","detail":"{read_only:false; response_revision:4494; number_of_response:1; }","duration":"301.764456ms","start":"2025-05-23T10:15:52.863134Z","end":"2025-05-23T10:15:53.164899Z","steps":["trace[1518140724] 'process raft request'  (duration: 82.137078ms)","trace[1518140724] 'compare'  (duration: 219.119648ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:15:53.164928Z","caller":"traceutil/trace.go:171","msg":"trace[1505998253] transaction","detail":"{read_only:false; response_revision:4495; number_of_response:1; }","duration":"281.256162ms","start":"2025-05-23T10:15:52.883659Z","end":"2025-05-23T10:15:53.164915Z","steps":["trace[1505998253] 'process raft request'  (duration: 281.061551ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:15:53.164959Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T10:15:52.863119Z","time spent":"301.808158ms","remote":"127.0.0.1:35814","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3258,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" mod_revision:4471 > success:<request_put:<key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" value_size:3198 >> failure:<request_range:<key:\"/registry/pods/default/node-k8s-app-6cc8f6c447-npkks\" > >"}
{"level":"warn","ts":"2025-05-23T10:15:53.454968Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.74165ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618684990054 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:4480 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:65 lease:3238535618684990051 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-23T10:15:53.455093Z","caller":"traceutil/trace.go:171","msg":"trace[1036665893] transaction","detail":"{read_only:false; response_revision:4496; number_of_response:1; }","duration":"245.247494ms","start":"2025-05-23T10:15:53.209827Z","end":"2025-05-23T10:15:53.455074Z","steps":["trace[1036665893] 'process raft request'  (duration: 124.299932ms)","trace[1036665893] 'compare'  (duration: 120.390428ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:15:53.476681Z","caller":"traceutil/trace.go:171","msg":"trace[284721748] transaction","detail":"{read_only:false; response_revision:4497; number_of_response:1; }","duration":"266.259412ms","start":"2025-05-23T10:15:53.210397Z","end":"2025-05-23T10:15:53.476657Z","steps":["trace[284721748] 'process raft request'  (duration: 265.988495ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:15:56.063497Z","caller":"traceutil/trace.go:171","msg":"trace[867737582] transaction","detail":"{read_only:false; response_revision:4501; number_of_response:1; }","duration":"196.09588ms","start":"2025-05-23T10:15:55.867383Z","end":"2025-05-23T10:15:56.063479Z","steps":["trace[867737582] 'process raft request'  (duration: 174.150206ms)","trace[867737582] 'compare'  (duration: 21.728761ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:16:00.132274Z","caller":"traceutil/trace.go:171","msg":"trace[1801679057] transaction","detail":"{read_only:false; response_revision:4508; number_of_response:1; }","duration":"267.200633ms","start":"2025-05-23T10:15:59.865052Z","end":"2025-05-23T10:16:00.132252Z","steps":["trace[1801679057] 'process raft request'  (duration: 224.905184ms)","trace[1801679057] 'compare'  (duration: 42.210244ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:16:03.673740Z","caller":"traceutil/trace.go:171","msg":"trace[829234693] transaction","detail":"{read_only:false; response_revision:4513; number_of_response:1; }","duration":"106.778886ms","start":"2025-05-23T10:16:03.566940Z","end":"2025-05-23T10:16:03.673718Z","steps":["trace[829234693] 'process raft request'  (duration: 42.194982ms)","trace[829234693] 'compare'  (duration: 64.450696ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:16:06.088022Z","caller":"traceutil/trace.go:171","msg":"trace[623545655] transaction","detail":"{read_only:false; response_revision:4517; number_of_response:1; }","duration":"221.941304ms","start":"2025-05-23T10:16:05.866062Z","end":"2025-05-23T10:16:06.088003Z","steps":["trace[623545655] 'process raft request'  (duration: 221.808096ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:16:13.224091Z","caller":"traceutil/trace.go:171","msg":"trace[1531073725] linearizableReadLoop","detail":"{readStateIndex:5193; appliedIndex:5192; }","duration":"200.517111ms","start":"2025-05-23T10:16:13.023557Z","end":"2025-05-23T10:16:13.224074Z","steps":["trace[1531073725] 'read index received'  (duration: 179.375692ms)","trace[1531073725] 'applied index is now lower than readState.Index'  (duration: 21.141019ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:16:13.224191Z","caller":"traceutil/trace.go:171","msg":"trace[642731700] transaction","detail":"{read_only:false; response_revision:4526; number_of_response:1; }","duration":"282.094401ms","start":"2025-05-23T10:16:12.942066Z","end":"2025-05-23T10:16:13.224161Z","steps":["trace[642731700] 'process raft request'  (duration: 260.92388ms)","trace[642731700] 'compare'  (duration: 21.00591ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:16:13.224224Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.65222ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/node-k8s-app-74d4cb658b-kq64j.1842207a08ff8fbb\" limit:1 ","response":"range_response_count:1 size:744"}
{"level":"info","ts":"2025-05-23T10:16:13.224262Z","caller":"traceutil/trace.go:171","msg":"trace[810833292] range","detail":"{range_begin:/registry/events/default/node-k8s-app-74d4cb658b-kq64j.1842207a08ff8fbb; range_end:; response_count:1; response_revision:4526; }","duration":"200.718123ms","start":"2025-05-23T10:16:13.023532Z","end":"2025-05-23T10:16:13.224250Z","steps":["trace[810833292] 'agreement among raft nodes before linearized reading'  (duration: 200.643119ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:16:13.224224Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"179.652009ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.58.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-05-23T10:16:13.224341Z","caller":"traceutil/trace.go:171","msg":"trace[1151018545] range","detail":"{range_begin:/registry/masterleases/192.168.58.2; range_end:; response_count:1; response_revision:4526; }","duration":"179.791518ms","start":"2025-05-23T10:16:13.044536Z","end":"2025-05-23T10:16:13.224327Z","steps":["trace[1151018545] 'agreement among raft nodes before linearized reading'  (duration: 179.65651ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:16:13.391051Z","caller":"traceutil/trace.go:171","msg":"trace[545580208] transaction","detail":"{read_only:false; response_revision:4528; number_of_response:1; }","duration":"104.147499ms","start":"2025-05-23T10:16:13.286883Z","end":"2025-05-23T10:16:13.391031Z","steps":["trace[545580208] 'process raft request'  (duration: 61.988568ms)","trace[545580208] 'compare'  (duration: 41.99342ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:16:15.971122Z","caller":"traceutil/trace.go:171","msg":"trace[2038659690] transaction","detail":"{read_only:false; response_revision:4531; number_of_response:1; }","duration":"106.481244ms","start":"2025-05-23T10:16:15.864616Z","end":"2025-05-23T10:16:15.971097Z","steps":["trace[2038659690] 'process raft request'  (duration: 106.160224ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:16:18.624622Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.258937ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618684990212 > lease_revoke:<id:2cf196fc982b32a0>","response":"size:28"}
{"level":"info","ts":"2025-05-23T10:16:23.260312Z","caller":"traceutil/trace.go:171","msg":"trace[883995634] transaction","detail":"{read_only:false; response_revision:4542; number_of_response:1; }","duration":"120.697988ms","start":"2025-05-23T10:16:23.139593Z","end":"2025-05-23T10:16:23.260291Z","steps":["trace[883995634] 'process raft request'  (duration: 98.740117ms)","trace[883995634] 'compare'  (duration: 21.839965ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:16:24.132935Z","caller":"traceutil/trace.go:171","msg":"trace[1691158431] transaction","detail":"{read_only:false; response_revision:4545; number_of_response:1; }","duration":"187.327238ms","start":"2025-05-23T10:16:23.945575Z","end":"2025-05-23T10:16:24.132903Z","steps":["trace[1691158431] 'process raft request'  (duration: 123.788038ms)","trace[1691158431] 'compare'  (duration: 63.408494ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:16:29.080989Z","caller":"traceutil/trace.go:171","msg":"trace[1118513111] transaction","detail":"{read_only:false; response_revision:4550; number_of_response:1; }","duration":"124.322107ms","start":"2025-05-23T10:16:28.956650Z","end":"2025-05-23T10:16:29.080972Z","steps":["trace[1118513111] 'process raft request'  (duration: 124.075989ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:16:48.696084Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.921683ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618684990397 > lease_revoke:<id:2cf196fc982b3360>","response":"size:28"}
{"level":"warn","ts":"2025-05-23T10:16:49.341136Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.203438ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:16:49.341247Z","caller":"traceutil/trace.go:171","msg":"trace[1182731367] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4578; }","duration":"121.353947ms","start":"2025-05-23T10:16:49.219877Z","end":"2025-05-23T10:16:49.341231Z","steps":["trace[1182731367] 'range keys from in-memory index tree'  (duration: 121.129334ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:16:49.595887Z","caller":"traceutil/trace.go:171","msg":"trace[1330697339] transaction","detail":"{read_only:false; response_revision:4579; number_of_response:1; }","duration":"135.85098ms","start":"2025-05-23T10:16:49.460001Z","end":"2025-05-23T10:16:49.595852Z","steps":["trace[1330697339] 'process raft request'  (duration: 135.444455ms)"],"step_count":1}


==> etcd [80f7b28aabb8] <==
{"level":"warn","ts":"2025-05-23T10:01:28.489963Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"177.123729ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:01:28.490164Z","caller":"traceutil/trace.go:171","msg":"trace[1644731924] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3296; }","duration":"177.388847ms","start":"2025-05-23T10:01:28.312763Z","end":"2025-05-23T10:01:28.490152Z","steps":["trace[1644731924] 'range keys from in-memory index tree'  (duration: 177.025223ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:01:33.063995Z","caller":"traceutil/trace.go:171","msg":"trace[1546770869] transaction","detail":"{read_only:false; response_revision:3301; number_of_response:1; }","duration":"145.593431ms","start":"2025-05-23T10:01:32.918379Z","end":"2025-05-23T10:01:33.063972Z","steps":["trace[1546770869] 'process raft request'  (duration: 145.474624ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:01:33.230804Z","caller":"traceutil/trace.go:171","msg":"trace[462286218] linearizableReadLoop","detail":"{readStateIndex:3772; appliedIndex:3771; }","duration":"287.041799ms","start":"2025-05-23T10:01:32.943742Z","end":"2025-05-23T10:01:33.230784Z","steps":["trace[462286218] 'read index received'  (duration: 120.225223ms)","trace[462286218] 'applied index is now lower than readState.Index'  (duration: 166.816076ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:01:33.230902Z","caller":"traceutil/trace.go:171","msg":"trace[1779380485] transaction","detail":"{read_only:false; response_revision:3302; number_of_response:1; }","duration":"305.164048ms","start":"2025-05-23T10:01:32.925723Z","end":"2025-05-23T10:01:33.230887Z","steps":["trace[1779380485] 'process raft request'  (duration: 263.23849ms)","trace[1779380485] 'compare'  (duration: 41.747546ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:01:33.230934Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"287.176307ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-05-23T10:01:33.230992Z","caller":"traceutil/trace.go:171","msg":"trace[1664146437] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:3302; }","duration":"287.287515ms","start":"2025-05-23T10:01:32.943697Z","end":"2025-05-23T10:01:33.230984Z","steps":["trace[1664146437] 'agreement among raft nodes before linearized reading'  (duration: 287.193209ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:01:33.231000Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"163.72208ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" limit:1 ","response":"range_response_count:1 size:744"}
{"level":"info","ts":"2025-05-23T10:01:33.231029Z","caller":"traceutil/trace.go:171","msg":"trace[1988053603] range","detail":"{range_begin:/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33; range_end:; response_count:1; response_revision:3302; }","duration":"163.786184ms","start":"2025-05-23T10:01:33.067234Z","end":"2025-05-23T10:01:33.231020Z","steps":["trace[1988053603] 'agreement among raft nodes before linearized reading'  (duration: 163.698079ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:01:33.231547Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T10:01:32.925690Z","time spent":"305.238452ms","remote":"127.0.0.1:50214","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3128,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/pods/default/node-k8s-app-74d4cb658b-l6mlr\" mod_revision:3208 > success:<request_put:<key:\"/registry/pods/default/node-k8s-app-74d4cb658b-l6mlr\" value_size:3068 >> failure:<request_range:<key:\"/registry/pods/default/node-k8s-app-74d4cb658b-l6mlr\" > >"}
{"level":"warn","ts":"2025-05-23T10:01:33.621975Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"208.634527ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618560248123 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" mod_revision:3268 > success:<request_put:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" value_size:640 lease:3238535618560247885 >> failure:<request_range:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-23T10:01:33.622158Z","caller":"traceutil/trace.go:171","msg":"trace[1908746472] linearizableReadLoop","detail":"{readStateIndex:3774; appliedIndex:3772; }","duration":"317.331819ms","start":"2025-05-23T10:01:33.304811Z","end":"2025-05-23T10:01:33.622143Z","steps":["trace[1908746472] 'read index received'  (duration: 108.473277ms)","trace[1908746472] 'applied index is now lower than readState.Index'  (duration: 208.858042ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:01:33.622159Z","caller":"traceutil/trace.go:171","msg":"trace[1508454712] transaction","detail":"{read_only:false; response_revision:3303; number_of_response:1; }","duration":"389.482694ms","start":"2025-05-23T10:01:33.232658Z","end":"2025-05-23T10:01:33.622141Z","steps":["trace[1508454712] 'process raft request'  (duration: 180.614151ms)","trace[1508454712] 'compare'  (duration: 208.541222ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:01:33.622232Z","caller":"traceutil/trace.go:171","msg":"trace[1895764528] transaction","detail":"{read_only:false; response_revision:3304; number_of_response:1; }","duration":"386.618812ms","start":"2025-05-23T10:01:33.235597Z","end":"2025-05-23T10:01:33.622216Z","steps":["trace[1895764528] 'process raft request'  (duration: 386.497505ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:01:33.622241Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T10:01:33.232636Z","time spent":"389.559498ms","remote":"127.0.0.1:50134","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":729,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" mod_revision:3268 > success:<request_put:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" value_size:640 lease:3238535618560247885 >> failure:<request_range:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" > >"}
{"level":"warn","ts":"2025-05-23T10:01:33.622280Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"317.467728ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:01:33.622330Z","caller":"traceutil/trace.go:171","msg":"trace[679834433] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3304; }","duration":"317.515431ms","start":"2025-05-23T10:01:33.304805Z","end":"2025-05-23T10:01:33.622320Z","steps":["trace[679834433] 'agreement among raft nodes before linearized reading'  (duration: 317.447227ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:01:33.622361Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T10:01:33.304750Z","time spent":"317.603036ms","remote":"127.0.0.1:50018","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-05-23T10:01:33.622284Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-23T10:01:33.235552Z","time spent":"386.704618ms","remote":"127.0.0.1:50196","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3300 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-05-23T10:01:35.408337Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.328401ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618560248133 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:3292 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:65 lease:3238535618560248130 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-23T10:01:35.409250Z","caller":"traceutil/trace.go:171","msg":"trace[1257156189] linearizableReadLoop","detail":"{readStateIndex:3776; appliedIndex:3775; }","duration":"227.221606ms","start":"2025-05-23T10:01:35.181817Z","end":"2025-05-23T10:01:35.409039Z","steps":["trace[1257156189] 'read index received'  (duration: 25.882041ms)","trace[1257156189] 'applied index is now lower than readState.Index'  (duration: 201.337765ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:01:35.409251Z","caller":"traceutil/trace.go:171","msg":"trace[2098102190] transaction","detail":"{read_only:false; response_revision:3305; number_of_response:1; }","duration":"295.675046ms","start":"2025-05-23T10:01:35.113543Z","end":"2025-05-23T10:01:35.409218Z","steps":["trace[2098102190] 'process raft request'  (duration: 94.090265ms)","trace[2098102190] 'compare'  (duration: 199.958978ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:01:35.409357Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"227.529525ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:01:35.409402Z","caller":"traceutil/trace.go:171","msg":"trace[760215412] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3305; }","duration":"227.577828ms","start":"2025-05-23T10:01:35.181809Z","end":"2025-05-23T10:01:35.409387Z","steps":["trace[760215412] 'agreement among raft nodes before linearized reading'  (duration: 227.476922ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:01:35.409501Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.451466ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:01:35.409548Z","caller":"traceutil/trace.go:171","msg":"trace[1178585450] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3305; }","duration":"111.526171ms","start":"2025-05-23T10:01:35.298009Z","end":"2025-05-23T10:01:35.409535Z","steps":["trace[1178585450] 'agreement among raft nodes before linearized reading'  (duration: 111.448466ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:01:35.816366Z","caller":"traceutil/trace.go:171","msg":"trace[1083417860] transaction","detail":"{read_only:false; response_revision:3306; number_of_response:1; }","duration":"157.837107ms","start":"2025-05-23T10:01:35.658509Z","end":"2025-05-23T10:01:35.816346Z","steps":["trace[1083417860] 'process raft request'  (duration: 157.710699ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:01:43.132115Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.1981ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618560248172 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/default/node-k8s-app-74d4cb658b-mbzfx\" mod_revision:3295 > success:<request_put:<key:\"/registry/pods/default/node-k8s-app-74d4cb658b-mbzfx\" value_size:3125 >> failure:<request_range:<key:\"/registry/pods/default/node-k8s-app-74d4cb658b-mbzfx\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-23T10:01:43.132202Z","caller":"traceutil/trace.go:171","msg":"trace[808085168] linearizableReadLoop","detail":"{readStateIndex:3785; appliedIndex:3784; }","duration":"185.18138ms","start":"2025-05-23T10:01:42.947008Z","end":"2025-05-23T10:01:43.132189Z","steps":["trace[808085168] 'read index received'  (duration: 60.760765ms)","trace[808085168] 'applied index is now lower than readState.Index'  (duration: 124.419715ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:01:43.132276Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.264685ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/node-k8s-app-74d4cb658b-mbzfx.18421f8bcbe123de\" limit:1 ","response":"range_response_count:1 size:744"}
{"level":"info","ts":"2025-05-23T10:01:43.132302Z","caller":"traceutil/trace.go:171","msg":"trace[1824624469] range","detail":"{range_begin:/registry/events/default/node-k8s-app-74d4cb658b-mbzfx.18421f8bcbe123de; range_end:; response_count:1; response_revision:3313; }","duration":"185.348591ms","start":"2025-05-23T10:01:42.946945Z","end":"2025-05-23T10:01:43.132293Z","steps":["trace[1824624469] 'agreement among raft nodes before linearized reading'  (duration: 185.281387ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:01:43.132320Z","caller":"traceutil/trace.go:171","msg":"trace[804701994] transaction","detail":"{read_only:false; response_revision:3313; number_of_response:1; }","duration":"206.477235ms","start":"2025-05-23T10:01:42.925830Z","end":"2025-05-23T10:01:43.132307Z","steps":["trace[804701994] 'process raft request'  (duration: 81.982915ms)","trace[804701994] 'compare'  (duration: 124.000488ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:01:45.054309Z","caller":"traceutil/trace.go:171","msg":"trace[1875834967] linearizableReadLoop","detail":"{readStateIndex:3789; appliedIndex:3788; }","duration":"107.389812ms","start":"2025-05-23T10:01:44.946892Z","end":"2025-05-23T10:01:45.054281Z","steps":["trace[1875834967] 'read index received'  (duration: 64.336681ms)","trace[1875834967] 'applied index is now lower than readState.Index'  (duration: 43.05203ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:01:45.054387Z","caller":"traceutil/trace.go:171","msg":"trace[772131937] transaction","detail":"{read_only:false; response_revision:3317; number_of_response:1; }","duration":"123.483132ms","start":"2025-05-23T10:01:44.930885Z","end":"2025-05-23T10:01:45.054368Z","steps":["trace[772131937] 'process raft request'  (duration: 80.278892ms)","trace[772131937] 'compare'  (duration: 42.960025ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:01:45.054453Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.538021ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" limit:1 ","response":"range_response_count:1 size:744"}
{"level":"info","ts":"2025-05-23T10:01:45.054522Z","caller":"traceutil/trace.go:171","msg":"trace[205461753] range","detail":"{range_begin:/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33; range_end:; response_count:1; response_revision:3317; }","duration":"107.654729ms","start":"2025-05-23T10:01:44.946853Z","end":"2025-05-23T10:01:45.054508Z","steps":["trace[205461753] 'agreement among raft nodes before linearized reading'  (duration: 107.51642ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:01:50.407345Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.599716ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:01:50.407467Z","caller":"traceutil/trace.go:171","msg":"trace[802134930] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3322; }","duration":"110.773426ms","start":"2025-05-23T10:01:50.296675Z","end":"2025-05-23T10:01:50.407448Z","steps":["trace[802134930] 'range keys from in-memory index tree'  (duration: 110.469106ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-23T10:01:54.167127Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.236596ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618560248237 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/node-k8s-app-74d4cb658b-mbzfx.18421f8bcbe123de\" mod_revision:3314 > success:<request_put:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-mbzfx.18421f8bcbe123de\" value_size:640 lease:3238535618560248232 >> failure:<request_range:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-mbzfx.18421f8bcbe123de\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-23T10:01:54.167250Z","caller":"traceutil/trace.go:171","msg":"trace[626887913] transaction","detail":"{read_only:false; response_revision:3327; number_of_response:1; }","duration":"163.647088ms","start":"2025-05-23T10:01:54.003584Z","end":"2025-05-23T10:01:54.167231Z","steps":["trace[626887913] 'process raft request'  (duration: 42.22048ms)","trace[626887913] 'compare'  (duration: 121.14549ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:01:56.168427Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.350103ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618560248254 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" mod_revision:3318 > success:<request_put:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" value_size:640 lease:3238535618560248232 >> failure:<request_range:<key:\"/registry/events/default/node-k8s-app-74d4cb658b-l6mlr.18421f8adb47ac33\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-23T10:01:56.168571Z","caller":"traceutil/trace.go:171","msg":"trace[692553485] linearizableReadLoop","detail":"{readStateIndex:3807; appliedIndex:3806; }","duration":"182.591991ms","start":"2025-05-23T10:01:55.985967Z","end":"2025-05-23T10:01:56.168559Z","steps":["trace[692553485] 'read index received'  (duration: 61.024874ms)","trace[692553485] 'applied index is now lower than readState.Index'  (duration: 121.565817ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:01:56.168665Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"182.705899ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/\" range_end:\"/registry/replicasets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-05-23T10:01:56.168709Z","caller":"traceutil/trace.go:171","msg":"trace[1434059788] range","detail":"{range_begin:/registry/replicasets/; range_end:/registry/replicasets0; response_count:0; response_revision:3331; }","duration":"182.772103ms","start":"2025-05-23T10:01:55.985929Z","end":"2025-05-23T10:01:56.168701Z","steps":["trace[1434059788] 'agreement among raft nodes before linearized reading'  (duration: 182.666896ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:01:56.168753Z","caller":"traceutil/trace.go:171","msg":"trace[1053374164] transaction","detail":"{read_only:false; response_revision:3331; number_of_response:1; }","duration":"200.348519ms","start":"2025-05-23T10:01:55.968378Z","end":"2025-05-23T10:01:56.168727Z","steps":["trace[1053374164] 'process raft request'  (duration: 78.649993ms)","trace[1053374164] 'compare'  (duration: 121.2886ms)"],"step_count":2}
{"level":"info","ts":"2025-05-23T10:01:56.562445Z","caller":"traceutil/trace.go:171","msg":"trace[1102104699] transaction","detail":"{read_only:false; response_revision:3332; number_of_response:1; }","duration":"153.061216ms","start":"2025-05-23T10:01:56.409363Z","end":"2025-05-23T10:01:56.562424Z","steps":["trace[1102104699] 'process raft request'  (duration: 152.897106ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:01:56.893953Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-05-23T10:01:56.894038Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}
{"level":"warn","ts":"2025-05-23T10:02:00.712425Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"368.690761ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238535618560248264 > lease_revoke:<id:2cf196fc90bbc97c>","response":"size:28"}
{"level":"info","ts":"2025-05-23T10:02:00.712618Z","caller":"traceutil/trace.go:171","msg":"trace[382430958] linearizableReadLoop","detail":"{readStateIndex:3809; appliedIndex:3808; }","duration":"533.274515ms","start":"2025-05-23T10:02:00.179324Z","end":"2025-05-23T10:02:00.712598Z","steps":["trace[382430958] 'read index received'  (duration: 164.459546ms)","trace[382430958] 'applied index is now lower than readState.Index'  (duration: 368.813369ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-23T10:02:00.712696Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"533.363521ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-23T10:02:00.712724Z","caller":"traceutil/trace.go:171","msg":"trace[229310554] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3332; }","duration":"533.398923ms","start":"2025-05-23T10:02:00.179316Z","end":"2025-05-23T10:02:00.712715Z","steps":["trace[229310554] 'agreement among raft nodes before linearized reading'  (duration: 533.34462ms)"],"step_count":1}
{"level":"info","ts":"2025-05-23T10:02:03.895243Z","caller":"etcdserver/server.go:1546","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"b2c6679ac05f2cf1","current-leader-member-id":"b2c6679ac05f2cf1"}
{"level":"warn","ts":"2025-05-23T10:02:03.895373Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-23T10:02:03.895401Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-23T10:02:03.895458Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-23T10:02:03.895476Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.58.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-05-23T10:02:03.965207Z","caller":"embed/etcd.go:613","msg":"stopping serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-05-23T10:02:03.965337Z","caller":"embed/etcd.go:618","msg":"stopped serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2025-05-23T10:02:03.965354Z","caller":"embed/etcd.go:410","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}


==> kernel <==
 10:16:50 up 54 min,  0 users,  load average: 1.65, 1.61, 1.76
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [6124a194e1e3] <==
W0523 10:02:02.556191       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:02.563316       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:02.586089       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:02.656957       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:02.795759       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:02.818077       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:04.801373       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.184281       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.232053       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.250024       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.318571       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.359878       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.368590       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.584876       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.607759       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.648218       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.656053       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.704718       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.781252       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.788078       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.819925       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.839105       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.857712       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.878850       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.904892       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.910024       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.926104       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.961935       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.988160       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:05.989554       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.005487       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.036275       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.036289       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.072114       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.117725       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.130631       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.138182       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.187166       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.209524       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.233039       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.264800       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.267937       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.297211       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.304427       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.323334       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.390401       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.458330       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.519506       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.588796       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.606191       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.634361       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.634374       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.643038       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.733809       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.804213       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.835857       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.854851       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.871358       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.902071       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0523 10:02:06.906460       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [95cdf6db8c85] <==
I0523 10:02:36.101795       1 customresource_discovery_controller.go:294] Starting DiscoveryController
I0523 10:02:36.102032       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0523 10:02:36.102049       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0523 10:02:36.101070       1 local_available_controller.go:156] Starting LocalAvailability controller
I0523 10:02:36.102112       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0523 10:02:36.102182       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0523 10:02:36.102231       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0523 10:02:36.102468       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0523 10:02:36.102506       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0523 10:02:36.102638       1 controller.go:119] Starting legacy_token_tracking_controller
I0523 10:02:36.102652       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0523 10:02:36.124352       1 controller.go:142] Starting OpenAPI controller
I0523 10:02:36.124768       1 controller.go:90] Starting OpenAPI V3 controller
I0523 10:02:36.124783       1 naming_controller.go:299] Starting NamingConditionController
I0523 10:02:36.124798       1 establishing_controller.go:81] Starting EstablishingController
I0523 10:02:36.124809       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0523 10:02:36.124824       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0523 10:02:36.124834       1 crd_finalizer.go:269] Starting CRDFinalizer
I0523 10:02:36.125155       1 repairip.go:200] Starting ipallocator-repair-controller
I0523 10:02:36.133483       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0523 10:02:36.195078       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0523 10:02:36.202671       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0523 10:02:36.211874       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0523 10:02:36.214619       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0523 10:02:36.214819       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0523 10:02:36.215198       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0523 10:02:36.215210       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0523 10:02:36.216427       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0523 10:02:36.216458       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0523 10:02:36.216520       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0523 10:02:36.216552       1 cache.go:39] Caches are synced for LocalAvailability controller
I0523 10:02:36.216841       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0523 10:02:36.216888       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0523 10:02:36.219959       1 aggregator.go:171] initial CRD sync complete...
I0523 10:02:36.219970       1 autoregister_controller.go:144] Starting autoregister controller
I0523 10:02:36.219976       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0523 10:02:36.219982       1 cache.go:39] Caches are synced for autoregister controller
I0523 10:02:36.291887       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0523 10:02:36.291933       1 policy_source.go:240] refreshing policies
I0523 10:02:36.292203       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0523 10:02:36.304243       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
E0523 10:02:36.844285       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0523 10:02:37.208450       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0523 10:02:44.125167       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0523 10:02:45.754208       1 controller.go:667] quota admission added evaluator for: endpoints
I0523 10:02:45.754209       1 controller.go:667] quota admission added evaluator for: endpoints
I0523 10:02:45.835723       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0523 10:02:45.973964       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0523 10:02:45.973965       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0523 10:07:58.987795       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0523 10:09:35.176522       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0523 10:09:41.444709       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0523 10:09:44.102136       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0523 10:09:44.548200       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
I0523 10:09:44.548333       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
W0523 10:09:45.586290       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.58.2]
I0523 10:09:45.836491       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0523 10:10:05.222898       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0523 10:10:05.687379       1 alloc.go:328] "allocated clusterIPs" service="default/node-k8s-service" clusterIPs={"IPv4":"10.107.210.49"}
I0523 10:12:36.155002       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [0520697c07e0] <==
I0523 09:54:37.341647       1 shared_informer.go:350] "Waiting for caches to sync" controller="stateful set"
I0523 09:54:37.344280       1 controllermanager.go:778] "Started controller" controller="root-ca-certificate-publisher-controller"
I0523 09:54:37.344323       1 controllermanager.go:730] "Controller is disabled by a feature gate" controller="resourceclaim-controller" requiredFeatureGates=["DynamicResourceAllocation"]
I0523 09:54:37.344355       1 controllermanager.go:756] "Warning: skipping controller" controller="storage-version-migrator-controller"
I0523 09:54:37.344373       1 controllermanager.go:741] "Warning: controller is disabled" controller="selinux-warning-controller"
I0523 09:54:37.344537       1 publisher.go:107] "Starting root CA cert publisher controller" logger="root-ca-certificate-publisher-controller"
I0523 09:54:37.344586       1 shared_informer.go:350] "Waiting for caches to sync" controller="crt configmap"
I0523 09:54:37.348959       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0523 09:54:37.362889       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0523 09:54:37.367124       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0523 09:54:37.368635       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0523 09:54:37.369832       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0523 09:54:37.371272       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0523 09:54:37.373962       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0523 09:54:37.374247       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0523 09:54:37.376950       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0523 09:54:37.390714       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0523 09:54:37.393137       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0523 09:54:37.395595       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0523 09:54:37.395931       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0523 09:54:37.396034       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0523 09:54:37.396194       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0523 09:54:37.396255       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0523 09:54:37.407671       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0523 09:54:37.410072       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0523 09:54:37.415445       1 shared_informer.go:357] "Caches are synced" controller="job"
I0523 09:54:37.415518       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0523 09:54:37.416659       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0523 09:54:37.417889       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0523 09:54:37.418022       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0523 09:54:37.418132       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0523 09:54:37.418153       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0523 09:54:37.428569       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0523 09:54:37.431040       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0523 09:54:37.434480       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0523 09:54:37.436925       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0523 09:54:37.438228       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0523 09:54:37.439527       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0523 09:54:37.445782       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0523 09:54:37.445874       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0523 09:54:37.448270       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0523 09:54:37.449674       1 shared_informer.go:357] "Caches are synced" controller="node"
I0523 09:54:37.449927       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0523 09:54:37.450078       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0523 09:54:37.450115       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0523 09:54:37.450133       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0523 09:54:37.461030       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0523 09:54:37.502250       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0523 09:54:37.567377       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0523 09:54:37.574568       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0523 09:54:37.634856       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0523 09:54:37.642385       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0523 09:54:37.649720       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0523 09:54:37.699536       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0523 09:54:37.740908       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0523 09:54:37.782689       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0523 09:54:38.170663       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0523 09:54:38.188287       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0523 09:54:38.188362       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0523 09:54:38.188377       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-controller-manager [0988443c33da] <==
I0523 10:02:45.614918       1 shared_informer.go:350] "Waiting for caches to sync" controller="TTL"
I0523 10:02:45.617507       1 controllermanager.go:778] "Started controller" controller="persistentvolume-protection-controller"
I0523 10:02:45.617648       1 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
I0523 10:02:45.617691       1 shared_informer.go:350] "Waiting for caches to sync" controller="PV protection"
I0523 10:02:45.619936       1 controllermanager.go:778] "Started controller" controller="ttl-after-finished-controller"
I0523 10:02:45.620135       1 ttlafterfinished_controller.go:112] "Starting TTL after finished controller" logger="ttl-after-finished-controller"
I0523 10:02:45.620184       1 shared_informer.go:350] "Waiting for caches to sync" controller="TTL after finished"
I0523 10:02:45.629540       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0523 10:02:45.642148       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0523 10:02:45.642899       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0523 10:02:45.651893       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0523 10:02:45.652036       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0523 10:02:45.656279       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0523 10:02:45.661598       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0523 10:02:45.663976       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0523 10:02:45.691811       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0523 10:02:45.691843       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0523 10:02:45.691904       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0523 10:02:45.691918       1 shared_informer.go:357] "Caches are synced" controller="node"
I0523 10:02:45.691924       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0523 10:02:45.691948       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0523 10:02:45.691997       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0523 10:02:45.692052       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0523 10:02:45.692095       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0523 10:02:45.692134       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0523 10:02:45.692147       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0523 10:02:45.692162       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0523 10:02:45.694395       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0523 10:02:45.696371       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0523 10:02:45.696541       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0523 10:02:45.697197       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0523 10:02:45.697824       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0523 10:02:45.704559       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0523 10:02:45.707199       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0523 10:02:45.713102       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0523 10:02:45.715486       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0523 10:02:45.719053       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0523 10:02:45.721893       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0523 10:02:45.740404       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0523 10:02:45.750624       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0523 10:02:45.781777       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0523 10:02:45.806786       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0523 10:02:45.873371       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0523 10:02:45.910233       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0523 10:02:45.910436       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0523 10:02:45.910699       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0523 10:02:45.910805       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0523 10:02:45.920919       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0523 10:02:45.924339       1 shared_informer.go:357] "Caches are synced" controller="job"
I0523 10:02:45.932682       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0523 10:02:45.978371       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0523 10:02:45.984133       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0523 10:02:46.003324       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0523 10:02:46.029422       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0523 10:02:46.029774       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0523 10:02:46.065246       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0523 10:02:46.443876       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0523 10:02:46.496126       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0523 10:02:46.496171       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0523 10:02:46.496185       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [435187e5c7bc] <==
I0523 10:02:34.602802       1 server_linux.go:63] "Using iptables proxy"
I0523 10:02:36.297788       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
E0523 10:02:36.298107       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0523 10:02:36.428578       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0523 10:02:36.428679       1 server_linux.go:145] "Using iptables Proxier"
I0523 10:02:36.438067       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0523 10:02:36.440105       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0523 10:02:36.442158       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0523 10:02:36.442383       1 server.go:516] "Version info" version="v1.33.1"
I0523 10:02:36.442450       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0523 10:02:36.453411       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0523 10:02:36.455944       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0523 10:02:36.458081       1 config.go:440] "Starting serviceCIDR config controller"
I0523 10:02:36.458139       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0523 10:02:36.458108       1 config.go:329] "Starting node config controller"
I0523 10:02:36.458218       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0523 10:02:36.458218       1 config.go:105] "Starting endpoint slice config controller"
I0523 10:02:36.458233       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0523 10:02:36.458260       1 config.go:199] "Starting service config controller"
I0523 10:02:36.458266       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0523 10:02:36.558847       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0523 10:02:36.558895       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0523 10:02:36.558916       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0523 10:02:36.558922       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-proxy [66ee3596c08f] <==
I0523 09:54:26.993266       1 server_linux.go:63] "Using iptables proxy"
I0523 09:54:29.324715       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
E0523 09:54:29.324797       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0523 09:54:29.443670       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0523 09:54:29.443753       1 server_linux.go:145] "Using iptables Proxier"
I0523 09:54:29.452255       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0523 09:54:29.455089       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0523 09:54:29.457351       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0523 09:54:29.457565       1 server.go:516] "Version info" version="v1.33.1"
I0523 09:54:29.457591       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0523 09:54:29.463324       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0523 09:54:29.465626       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0523 09:54:29.467269       1 config.go:199] "Starting service config controller"
I0523 09:54:29.467283       1 config.go:105] "Starting endpoint slice config controller"
I0523 09:54:29.467317       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0523 09:54:29.467318       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0523 09:54:29.467346       1 config.go:440] "Starting serviceCIDR config controller"
I0523 09:54:29.467363       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0523 09:54:29.467690       1 config.go:329] "Starting node config controller"
I0523 09:54:29.467734       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0523 09:54:29.567856       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0523 09:54:29.567921       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0523 09:54:29.567996       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0523 09:54:29.625661       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [1a01f895e229] <==
I0523 10:02:27.225024       1 serving.go:386] Generated self-signed cert in-memory
W0523 10:02:27.804433       1 authentication.go:397] Error looking up in-cluster authentication configuration: Get "https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": dial tcp 192.168.58.2:8443: connect: connection refused
W0523 10:02:27.804466       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0523 10:02:27.804473       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0523 10:02:27.811069       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0523 10:02:27.811100       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0523 10:02:27.812682       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0523 10:02:27.812737       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0523 10:02:27.812842       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0523 10:02:27.812900       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0523 10:02:27.813310       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0523 10:02:27.813311       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0523 10:02:27.813361       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0523 10:02:27.813393       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0523 10:02:27.813397       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0523 10:02:27.813412       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0523 10:02:27.813442       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0523 10:02:27.813477       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0523 10:02:27.813511       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0523 10:02:27.813522       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0523 10:02:27.813553       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0523 10:02:27.813653       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0523 10:02:27.813687       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0523 10:02:27.813693       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0523 10:02:27.813726       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0523 10:02:27.813743       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0523 10:02:28.627376       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0523 10:02:28.675762       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0523 10:02:28.692948       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0523 10:02:28.702357       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0523 10:02:28.804049       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0523 10:02:28.845646       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0523 10:02:28.875625       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0523 10:02:28.889734       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0523 10:02:28.930473       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0523 10:02:29.140412       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0523 10:02:29.142337       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0523 10:02:29.171767       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0523 10:02:29.292582       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0523 10:02:29.304756       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0523 10:02:29.336586       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0523 10:02:29.389076       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.58.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0523 10:02:36.193892       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0523 10:02:36.303210       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0523 10:02:36.303337       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0523 10:02:36.303433       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0523 10:02:36.303535       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0523 10:02:36.303684       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0523 10:02:36.303869       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0523 10:02:36.304134       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0523 10:02:36.305617       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0523 10:02:36.305913       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0523 10:02:36.306351       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0523 10:02:36.306958       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0523 10:02:36.307179       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0523 10:02:36.307392       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found, clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0523 10:02:36.306980       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io \"system:discovery\" not found, clusterrole.rbac.authorization.k8s.io \"system:kube-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:volume-scheduler\" not found, clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found, clusterrole.rbac.authorization.k8s.io \"system:basic-user\" not found]" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I0523 10:02:40.013389       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [4371e7b3876d] <==
I0523 09:54:26.290720       1 serving.go:386] Generated self-signed cert in-memory
I0523 09:54:29.426974       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0523 09:54:29.427037       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0523 09:54:29.623785       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0523 09:54:29.623843       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0523 09:54:29.623862       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0523 09:54:29.623873       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0523 09:54:29.623782       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0523 09:54:29.623964       1 shared_informer.go:350] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I0523 09:54:29.624316       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0523 09:54:29.624348       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0523 09:54:29.724656       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0523 09:54:29.724672       1 shared_informer.go:357] "Caches are synced" controller="RequestHeaderAuthRequestController"
I0523 09:54:29.724623       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0523 10:01:56.820228       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
E0523 10:01:56.820247       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
May 23 10:15:05 minikube kubelet[2380]: I0523 10:15:05.869143    2380 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="90923a6c-c9d3-4e93-b727-19627caea597" path="/var/lib/kubelet/pods/90923a6c-c9d3-4e93-b727-19627caea597/volumes"
May 23 10:15:05 minikube kubelet[2380]: I0523 10:15:05.924193    2380 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2zk6f\" (UniqueName: \"kubernetes.io/projected/299d9f13-a81a-437f-89d2-562044087c9e-kube-api-access-2zk6f\") pod \"node-k8s-app-74d4cb658b-8vj5b\" (UID: \"299d9f13-a81a-437f-89d2-562044087c9e\") " pod="default/node-k8s-app-74d4cb658b-8vj5b"
May 23 10:15:09 minikube kubelet[2380]: I0523 10:15:09.789469    2380 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6138ee1693553cbd136bde73ac1ab2683ccb10e3a6cb06370a90a0c9e2be989b"
May 23 10:15:10 minikube kubelet[2380]: I0523 10:15:10.960725    2380 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gbj92\" (UniqueName: \"kubernetes.io/projected/cf56bdac-1a40-41fd-b659-7a530a343648-kube-api-access-gbj92\") pod \"node-k8s-app-6cc8f6c447-npkks\" (UID: \"cf56bdac-1a40-41fd-b659-7a530a343648\") " pod="default/node-k8s-app-6cc8f6c447-npkks"
May 23 10:15:12 minikube kubelet[2380]: I0523 10:15:12.865392    2380 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e50c2b80ca090b42cc717b571f52e0c63d6fabee8dae94393f2052ac2f3a9cc0"
May 23 10:15:13 minikube kubelet[2380]: E0523 10:15:13.636956    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:13 minikube kubelet[2380]: E0523 10:15:13.637016    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:13 minikube kubelet[2380]: E0523 10:15:13.637294    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nf2v6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-74d4cb658b-kq64j_default(141b0d03-83b9-49b1-a8bd-d8e46ba66617): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:15:13 minikube kubelet[2380]: E0523 10:15:13.639213    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-kq64j" podUID="141b0d03-83b9-49b1-a8bd-d8e46ba66617"
May 23 10:15:13 minikube kubelet[2380]: E0523 10:15:13.882309    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-kq64j" podUID="141b0d03-83b9-49b1-a8bd-d8e46ba66617"
May 23 10:15:17 minikube kubelet[2380]: E0523 10:15:17.346585    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:17 minikube kubelet[2380]: E0523 10:15:17.346658    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:17 minikube kubelet[2380]: E0523 10:15:17.346904    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2zk6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-74d4cb658b-8vj5b_default(299d9f13-a81a-437f-89d2-562044087c9e): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:15:17 minikube kubelet[2380]: E0523 10:15:17.348512    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-8vj5b" podUID="299d9f13-a81a-437f-89d2-562044087c9e"
May 23 10:15:17 minikube kubelet[2380]: E0523 10:15:17.912442    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-8vj5b" podUID="299d9f13-a81a-437f-89d2-562044087c9e"
May 23 10:15:20 minikube kubelet[2380]: E0523 10:15:20.998334    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:20 minikube kubelet[2380]: E0523 10:15:20.998414    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:20 minikube kubelet[2380]: E0523 10:15:20.998536    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbj92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-6cc8f6c447-npkks_default(cf56bdac-1a40-41fd-b659-7a530a343648): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:15:20 minikube kubelet[2380]: E0523 10:15:20.999789    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-6cc8f6c447-npkks" podUID="cf56bdac-1a40-41fd-b659-7a530a343648"
May 23 10:15:21 minikube kubelet[2380]: E0523 10:15:21.944951    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-6cc8f6c447-npkks" podUID="cf56bdac-1a40-41fd-b659-7a530a343648"
May 23 10:15:30 minikube kubelet[2380]: E0523 10:15:30.548172    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:30 minikube kubelet[2380]: E0523 10:15:30.548242    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:30 minikube kubelet[2380]: E0523 10:15:30.548424    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nf2v6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-74d4cb658b-kq64j_default(141b0d03-83b9-49b1-a8bd-d8e46ba66617): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:15:30 minikube kubelet[2380]: E0523 10:15:30.550179    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-kq64j" podUID="141b0d03-83b9-49b1-a8bd-d8e46ba66617"
May 23 10:15:34 minikube kubelet[2380]: E0523 10:15:34.264322    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:34 minikube kubelet[2380]: E0523 10:15:34.264436    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:34 minikube kubelet[2380]: E0523 10:15:34.264683    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2zk6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-74d4cb658b-8vj5b_default(299d9f13-a81a-437f-89d2-562044087c9e): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:15:34 minikube kubelet[2380]: E0523 10:15:34.266023    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-8vj5b" podUID="299d9f13-a81a-437f-89d2-562044087c9e"
May 23 10:15:38 minikube kubelet[2380]: E0523 10:15:38.427099    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:38 minikube kubelet[2380]: E0523 10:15:38.427170    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:38 minikube kubelet[2380]: E0523 10:15:38.427292    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbj92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-6cc8f6c447-npkks_default(cf56bdac-1a40-41fd-b659-7a530a343648): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:15:38 minikube kubelet[2380]: E0523 10:15:38.428578    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-6cc8f6c447-npkks" podUID="cf56bdac-1a40-41fd-b659-7a530a343648"
May 23 10:15:43 minikube kubelet[2380]: E0523 10:15:43.859542    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-kq64j" podUID="141b0d03-83b9-49b1-a8bd-d8e46ba66617"
May 23 10:15:45 minikube kubelet[2380]: E0523 10:15:45.858657    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-8vj5b" podUID="299d9f13-a81a-437f-89d2-562044087c9e"
May 23 10:15:52 minikube kubelet[2380]: E0523 10:15:52.857330    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-6cc8f6c447-npkks" podUID="cf56bdac-1a40-41fd-b659-7a530a343648"
May 23 10:15:59 minikube kubelet[2380]: E0523 10:15:59.528046    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:59 minikube kubelet[2380]: E0523 10:15:59.528118    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:15:59 minikube kubelet[2380]: E0523 10:15:59.528237    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nf2v6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-74d4cb658b-kq64j_default(141b0d03-83b9-49b1-a8bd-d8e46ba66617): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:15:59 minikube kubelet[2380]: E0523 10:15:59.529447    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-kq64j" podUID="141b0d03-83b9-49b1-a8bd-d8e46ba66617"
May 23 10:16:03 minikube kubelet[2380]: E0523 10:16:03.514473    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:16:03 minikube kubelet[2380]: E0523 10:16:03.514560    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:16:03 minikube kubelet[2380]: E0523 10:16:03.514708    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2zk6f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-74d4cb658b-8vj5b_default(299d9f13-a81a-437f-89d2-562044087c9e): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:16:03 minikube kubelet[2380]: E0523 10:16:03.516398    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-8vj5b" podUID="299d9f13-a81a-437f-89d2-562044087c9e"
May 23 10:16:09 minikube kubelet[2380]: E0523 10:16:09.518358    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:16:09 minikube kubelet[2380]: E0523 10:16:09.518441    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:16:09 minikube kubelet[2380]: E0523 10:16:09.518612    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbj92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-6cc8f6c447-npkks_default(cf56bdac-1a40-41fd-b659-7a530a343648): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:16:09 minikube kubelet[2380]: E0523 10:16:09.519890    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-6cc8f6c447-npkks" podUID="cf56bdac-1a40-41fd-b659-7a530a343648"
May 23 10:16:12 minikube kubelet[2380]: E0523 10:16:12.856799    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-kq64j" podUID="141b0d03-83b9-49b1-a8bd-d8e46ba66617"
May 23 10:16:15 minikube kubelet[2380]: E0523 10:16:15.857265    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-8vj5b" podUID="299d9f13-a81a-437f-89d2-562044087c9e"
May 23 10:16:20 minikube kubelet[2380]: E0523 10:16:20.855750    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-6cc8f6c447-npkks" podUID="cf56bdac-1a40-41fd-b659-7a530a343648"
May 23 10:16:23 minikube kubelet[2380]: E0523 10:16:23.855555    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-kq64j" podUID="141b0d03-83b9-49b1-a8bd-d8e46ba66617"
May 23 10:16:30 minikube kubelet[2380]: E0523 10:16:30.855303    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-8vj5b" podUID="299d9f13-a81a-437f-89d2-562044087c9e"
May 23 10:16:31 minikube kubelet[2380]: E0523 10:16:31.855293    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-6cc8f6c447-npkks" podUID="cf56bdac-1a40-41fd-b659-7a530a343648"
May 23 10:16:35 minikube kubelet[2380]: E0523 10:16:35.855349    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-kq64j" podUID="141b0d03-83b9-49b1-a8bd-d8e46ba66617"
May 23 10:16:41 minikube kubelet[2380]: E0523 10:16:41.856052    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-8vj5b" podUID="299d9f13-a81a-437f-89d2-562044087c9e"
May 23 10:16:46 minikube kubelet[2380]: E0523 10:16:46.855206    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ImagePullBackOff: \"Back-off pulling image \\\"node-k8s-app\\\": ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-6cc8f6c447-npkks" podUID="cf56bdac-1a40-41fd-b659-7a530a343648"
May 23 10:16:51 minikube kubelet[2380]: E0523 10:16:51.590270    2380 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:16:51 minikube kubelet[2380]: E0523 10:16:51.590345    2380 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="node-k8s-app:latest"
May 23 10:16:51 minikube kubelet[2380]: E0523 10:16:51.590481    2380 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:node-k8s-app,Image:node-k8s-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nf2v6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod node-k8s-app-74d4cb658b-kq64j_default(141b0d03-83b9-49b1-a8bd-d8e46ba66617): ErrImagePull: Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 23 10:16:51 minikube kubelet[2380]: E0523 10:16:51.591750    2380 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-k8s-app\" with ErrImagePull: \"Error response from daemon: pull access denied for node-k8s-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/node-k8s-app-74d4cb658b-kq64j" podUID="141b0d03-83b9-49b1-a8bd-d8e46ba66617"


==> storage-provisioner [231c33f22508] <==
I0523 10:02:30.021137       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0523 10:02:30.023331       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [9d1a23583570] <==
W0523 10:15:52.345733       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:15:52.371753       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:15:54.375178       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:15:54.400730       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:15:56.404239       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:15:56.429808       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:15:58.433494       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:15:58.459061       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:00.462309       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:00.508541       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:02.511798       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:02.536366       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:04.540033       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:04.566095       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:06.568904       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:06.595728       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:08.598352       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:08.646280       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:10.649412       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:10.675439       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:12.678998       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:12.726465       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:14.729656       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:14.755326       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:16.757150       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:16.782504       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:18.785635       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:18.810999       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:20.815770       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:20.842004       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:22.844688       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:22.870119       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:24.873499       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:24.921472       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:26.924816       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:26.951230       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:28.954392       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:29.082085       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:31.084860       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:31.110634       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:33.113736       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:33.138508       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:35.141517       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:35.190556       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:37.195127       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:37.242557       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:39.246082       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:39.271994       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:41.275443       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:41.322783       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:43.326512       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:43.352148       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:45.355245       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:45.403136       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:47.404767       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:47.454571       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:49.457831       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:49.597150       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:51.742801       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0523 10:16:51.959484       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

